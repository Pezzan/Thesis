\documentclass[12pt, a4paper, twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{eso-pic}
\usepackage{wrapfig}
\usepackage{lineno}
\usepackage{tikz}
\usepackage{booktabs}
\usetikzlibrary{shapes.geometric, arrows}
\linenumbers

\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]


\newcommand\AlCentroPagina[1]{%
\AddToShipoutPicture*{\AtPageCenter{%
\makebox(0,0){\includegraphics%
[width=1.3\paperwidth]{#1}}}}}

\makeatletter
% Une commande sembleble à \rlap ou \llap, mais centrant son argument
\def\clap#1{\hbox to 0pt{\hss #1\hss}}%
% Une commande centrant son contenu (à utiliser en mode vertical)
\def\ligne#1{%
  \hbox to \hsize{%
    \vbox{\centering #1}}}%
% Une comande qui met son premier argument à gauche, le second au 
% milieu et le dernier à droite, la première ligne ce chacune de ces
% trois boites coïncidant
\def\haut#1#2#3{%
  \hbox to \hsize{%
    \rlap{\vtop{\raggedright #1}}%
    \hss
    \clap{\vtop{\centering #2}}%
    \hss
    \llap{\vtop{\raggedleft #3}}}}%
% Idem, mais cette fois-ci, c'est la dernière ligne
\def\bas#1#2#3{%
  \hbox to \hsize{%
    \rlap{\vbox{\raggedright #1}}%
    \hss
    \clap{\vbox{\centering #2}}%
    \hss
    \llap{\vbox{\raggedleft #3}}}}%
% La commande \maketitle
\def\maketitle{%
  \thispagestyle{empty}\vbox to \vsize{%
    \haut{}{\@blurb}{}
    \vfill
    \ligne{\LARGE \bf\@title}
    \vspace{5mm}
    \ligne{\Large \@author}
    \vspace{1mm}\ligne{\texttt{<\@email>}}
    \vspace{1cm}
    \vfill
    \vfill
    \bas{}{\@location, \@date}{}
    }%
  \cleardoublepage
  }
% Les commandes permettant de définir la date, le lieu, etc.
\def\date#1{\def\@date{#1}}
\def\author#1{\def\@author{#1}}
\def\title#1{\def\@title{#1}}
\def\location#1{\def\@location{#1}}
\def\blurb#1{\def\@blurb{#1}}
\def\email#1{\def\@email{#1}}
% Valeurs par défaut
\date{\today}
\author{}
\title{}
\location{Pisa}
\blurb{}
\email{no email address}
\makeatother
  \title{Research on the dosimetric accuracy of Fine Sampling  for radiation therapy treatment planning}
  \author{Giuseppe \textsc{Pezzano}}
  \email{gpp.pezzano@gmail.com}
  \date{19 July 2018}
  \location{Pisa}
  \blurb{Università di Pisa\\
  Dipartimento di Fisica Enrico Fermi\\
  and\\
  Deutsches Krebsforschungszentrum\\
  DKFZ Heidelberg\\
  ~\\
  internal supervisor:\\
  \large{Prof. Alberto Del Guerra}\\
  ~\\
  supervisor:\\
  \large{Dr. Mark Bangert}}
    
\begin{document}
\newpage
\begin{titlepage}
\centering
\AlCentroPagina{Images/unipi.jpg}
\maketitle
\end{titlepage}

\newpage

\newenvironment{abstract}%
{\newpage\thispagestyle{empty}\null\vspace{\stretch{1}}\begin{center}\textbf{Abstract}\\[20pt]}%
{\end{center}\vspace{\stretch{1}}\null}%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage


\begin{abstract}
The hadrontherapy is a medical therapy based on a new technology allowing to treat cancer without surgery and with contained damages to healthy surrounding tissues. Nowadays, before every treatment, some simulations are run in order to estimate the dose deposition inside the patient. The most accurate softwares for this purpose are Monte Carlo based and simulate the effects for a huge amount of particles requiring a significant amount of computing time. Lately, new methods have been developed, such as the Analytical Probabilistic Modelling, the Pencil Beam Algorithms and the Fine Sampling Beam. The aim of this project is to understand, improve and implement the last two of the aforesaid methods by using MatRad, and to compare them with the measurements provided by the Heidelberg Ions Therapy Center, the Syngo (Siemens' MatRad clone) and the Fluka-based MC simulations. The most peculiar property of the Pencil Beam Algorithms is the speed, and I figured out that they can be exploited in a new method in order to increase the accuracy of the simulation instead. Then, my work focuses on the realization of this new software based on the Pencil Beam Algorithms. The first of the three steps of this work is to compare MatRad, the Fine sampling (FS) algorithm and the Monte Carlo simulations for simple cases, i.e., in water environment and with some inhomogeneities. Then, I show the results for some cases of the Spread-Out Bragg Peaks and finally, for some full fields simulations either for Matrad and FS and Syngo. 

As I show in this work... (preliminary rough overview of the result)

\end{abstract}

\tableofcontents

\chapter{Introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{History of charged particle therapy}

Nowadays, one of the most used therapies to treat a patient affected by cancer is radiation therapy. Radiation therapy is a medical method using ionizing radiation to kill the tumor. The tumor cells are destroyed by beams of X-rays (high energy photons) produced by the acceleration of electrons onto a metal anode where, trough Bremsstrahlung, X-rays are emitted and then directly delivered to the patient. 
This process uses crossing beams from many angles and it is planned such that the tumor target is hit by the radiation
\begin{wrapfigure}{r}{.5\textwidth}
\centering
{\includegraphics[width=.5\textwidth]{Images/37cycl}}
\caption{E. O. Lawrence (right) and M.S. Livingston (left) standing beside the 37-inch
cyclotron (Berkeley Lab). From \emph{The evolution of the cyclotron} by Lawrence E.O. -
Nobel Lecture, 1951}
\label{fig:37cycl}
\vspace{-5mm}
\end{wrapfigure}
\noindent while the surrounding normal tissues stay preserved. Nevertheless, some radiation dose is always deposited in the healthy tissues.
When the irradiating beams consist of charged particles (protons, carbon and other ions), radiation therapy is named hadrontherapy. The physical and radiobiological properties of these charged particle make the main strength of the hadrontherapy.
The introduction of charged particles, used as an alternative to X-rays determined a fundamental improvement. In fact, charged particles can penetrate the tissues with a smaller diffusion and deposit the peak of energy just before stopping. This allows to deposit more dose in the tumor than in the surrounding tissues by using just one beam.
The peaked shape of the hadron energy deposition is called Bragg peak and has become the symbol of hadrontherapy. With the use of hadrons the tumour can be irradiated while the damage to healthy tissues is contained and, in most cases, far smaller than with X-rays. Today radiotherapy is actuated with both photons and ions.
The first hadrons used in radiotherapy were neutrons because of their large range inside matter. Neutrons ionizate via the recoil of the ions they scatter with. In biological tissues these ions are mostly termal-energy protons and they have a great Relative Biological Effectiveness (RBE). 
The first experiments started in 1936 and were published by the Lawrence brothers\footnote{Lawrence JH, Aebersold PC, Lawrence EO. Proc Natl Acad Sci U S A 1936;22(9):543.} and, in 1938, the first patients were treated with fast neutrons produced by bombarding a beryllium target with deuterions accelerated by a 37-inch cyclotron (shown in figure \ref{fig:37cycl}) up to $8\,MeV$. The neutrons, inside matter, transfer their energy mainly ionizing atoms. The secondary electrons produced in nuclear interaction have a Linear Energy Transfer (LET) value much larger than the elctrons freed by photons in conventional radiotherapy.
Because of the not suitable depth-dose distribution, the biological effects in the normal tissues outside of the target volume were large. Therefore, because of the severe side effects, neutron therapy has been terminated.
The next experiment was done with negative-pion beams producing an additional boost of dose at the end of their range. However, clinical trials could not find an improved cure rate and the pion treatments were terminated worldwide after the treatment of some 800 patients.\footnote{H. Blattmann, Pions at Los Alamos, PSI and Vancouver, in Hadrontherapy in Oncology, eds. U. Amaldi and B. Larsson (Elsevier, 1994), pp. 199–207.}

The pioneer of proton therapy was the physicist Robert Wilson, who in 1946 proposed to use protons for cancer treatment and later founded and directed the Fermi National Accelerator Laboratory (Fermilab) near Chicago.
In the 50s, patients started to be treated in nuclear physics research facilities using accelerator that were not built for this purpose.
The first accelerators were not powerful enough to allow treatment deep inside the patients; thus at the beginning, this method was applied only to few parts of the body and for the most external tumors.
In the 70s, thanks to the improvements in accelerator technology and medical imaging and computers, proton therapy became a valid treatment.
Before 80s, there were not any dedicated facilities for particle therapy. In fact, this kind of treatment was done in nuclear physics research centers, using easy-to-build horizontal beam lines.

But this field could not develop without the constrution of dedicated facilities. 
As Myers and Bruning say in \cite{mye:acc}: \emph{From the sixties to the eighties of the 20th century, particle radiotherapy was based exclusively on accelerator facilities developed for nuclear physics, with beam lines and treatment rooms adapted to the needs of radiotherapy. The first hospital based installations occurred: at first, the MC60, a $62.5\,MeV$ proton cyclotron, delivered by Scanditronix, operating at the Clatterbridge Oncology Centre (UK) since 1989; and then from 1990, a dedicated $250\,MeV$ proton synchrotron, developed by FermiLab at Loma Linda University (California, USA), the first dedicated clinical facility equipped with three rotating gantries.}
The MC60 cyclotron has been used for fast neutron radiotherapy and is still used for the treatment of ocular tumours.\footnote{Kacperek A. Protontherapy of eye tumours in the UK: a review of treatment at
Clatterbridge. Appl Radiat Isotopes 2009;67(3):378e86.} In the same period, six more centres were opened treating eye melanomas and other eye tumours and malformations featuring a sigle horizontal beam. These accelerators had a maximum energy of $60-70\,MeV$ and were located in PSI, Nice, University California San Francisco, Triumph, Berlin and Catania.
\begin{figure}[!t]
\centering
{\includegraphics[width=\textwidth]{Images/Synch_fermilab}}
\caption{The LLUMC synchrotron built in Fermilab. From Slater \cite{jd:llu}}
\label{fig:synchF}
\end{figure}
As already said, the major introduction at that time was the 7-m-diameter $250\,MeV$ synchrotron built by FermiLab, (figure \ref{fig:synchF}) and installed at Loma Linda University in California. This was coupled with three $10\,m$ diameter rotating gantries used in order to adjust the entering angle of the beam in respect to the patient's body.
This was the first synchrotron applied to a hospital-based facility\footnote{Slater JM, Archambeau JO, Miller DW, Notarus MI, Preston W, Slater JD. The proton treatment center at Loma Linda University Medical Centre: rationale
for and description of its development. Int J Radiat Oncol Biol Phys 1992;22: 383e9}.
Another major advancement in particle therapy, which I will talk about later, was the application of scanning beams, which allows painting the tumour target with the Bragg peak.
The first scanning systems were developed in the research facilities at PSI (Villigen, Switzerland) for protons and GSI (Darmstadt, Germany) for carbon ions and have been used to treat patients since 1996 and 1997, respectively.
In order to use the accelerator at its maximum possibility every hospital centre has usually between 3 and 5 gantries (or treatment rooms) so that, during the 3-5 minutes irradiation in one room, in others the procedures of alignment to prepare other patients can be started. Because each patient needs 20-30 sessions, one four-room centre can treat up to 1200-1300 patients a year. The size of the facility and the optimisation of the workflow are still debated and many studies have addressed this problem.

As stated on Particle Therapy Co-Operative Group\footnote{\url{https://www.ptcog.ch/}}: \emph{At present about forty-five proton therapy centres are in operation or under construction throughout the world. The list of the centres in operation with the statistics of the number of treated patients is updated every year by the PTCOG.}
%%%   includere tabella dei centri magari

\emph{In Europe, the interest in hadrontherapy has been growing rapidly and the first dual ion (carbon and protons) clinical facility in Heidelberg, Germany started treating patients at the end of 2009. Some more such facilities are now in operation, for example: CNAO in Pavia, MIT in Marburg, and MedAustron in Wiener Neustadt are treating patients.
Globally there is a huge momentum in particle therapy, especially treatment with protons. By 2020 it is expected there will be almost 100 centres around the world, with over 30 of these in Europe}.


\section{Production of therapeutic charged particle beams}
The core component in the production of therapeutic charged particle beams is the accelerator. Some centres use a \emph{Cyclotron}, smaller than a \emph{Synchrotron}, which allows to accelerate protons to energies sufficient for the therapies but, because of technology limits, today this is not used for light ions therapy. But research is improving fast on this topic and there are some prototypes of cyclotrons for carbon Ions therapy, as the IBA C400 (more info in \cite{jong:iba}). In order to accelerate Carbon or Helium ions to energies around some hundreds of $MeV$ per nucleon, there is the need of a \emph{Synchrotron}. The most modern centres use a circular accelerator, usually around tens of meters of length. Meanwhile the patient is positioned on a couch which is able to rotate so that the beam can hit the patient with different angles. There are two different possible environments: \emph{i)} the standard \emph{fixed beam line} that has a static beam direction and a system that allows the couch (on which the patient lays down) to cover a $360^\circ$ angle on the horizontal plane and eventually of small angles on the azimuthal plane; \emph{ii)} the \emph{gantry} that has a system of magnets, that rotating around the patient, bends the beam and allows covering the full $4\pi$ angle, as for example the one at the Heidelberg Ion-Beam Therapy Center (HIT) (figure \ref{fig:HIT}). The gantry at HIT is a rotating construction made of steel that is 25 meters long, 13 meters in diameter and weighs 670 tons (the weight of an AirBus A380 fully charged is around 577 tons), of which 600 tons can be rotated with a precision below one millimetre. 
%\begin{figure}[t]
%{\includegraphics[width=.45\textwidth]{Images/gantryHITout2c}}
%{\includegraphics[width=.45\textwidth]{Images/gantryHITout}}\\
%{\includegraphics[width=.45\textwidth]{Images/gantryHITin}}
%\caption{The 25-meter-long $360^\circ$ rotating gantry at HIT, from the inside and the outside.}
%\label{fig:HIT}
%\end{figure}

\begin{figure}[t]
\includegraphics[width=\textwidth]{Images/gantryHIT}
\caption{The 25-meter-long $360^\circ$ rotating gantry at HIT, from the inside and the outside. From the websites of Univerit\"atsKlinicum Heidelberg \url{https://www.klinikum.uni-heidelberg.de} and DKFZ \url{https://www.dkfz.de}}
\label{fig:HIT}
\end{figure}

In the case of a center based on Synchrotron, the path of the particle starts from the source, then it is accelerated and injected inside the main ring where particles are futhermore accelerated in bunches. After the part of acceleration, the Synchrotron can be used as a storage ring where all the bunches, of fixed energy and intensity, wait for the extraction. Extracted particles will fly inside a vacuum tube from the ring to the gantry. During this flight, there is a part of beam diagnostic, where intensity, position and dimension of the beam is analysed to detect and avoid abnormalities. Using \emph{Active Beam Scanning} (of which I will talk in section \ref{activeScanning} ), shape and direction are adjusted with the use of lead collimators and two magnets deflecting the beams on the plane perpendicular to the flight direction. After the last step, the beam exits the machine and enters the patient.

\section{Interaction of charged particles with matter}
Many effects, involving beam particles and patient tissue, must be taken in account during a treatment.
The main ones are electromagnetic interactions with electrons that constitute the primary cause of energy loss by protons. Because they have a mass which is large compared to the mass of the electrons, they lose only a small fraction of their energy in a single interaction (at most $4 m/M = 0.0022\%$, where $m$ is the electron mass and $M$ is the proton mass) and they are deflected by only small angles in each interaction.
In general, the proton interactions with matter can be divided into three categories: interactions with the individual electrons of atoms; interactions with the nucleus; and interactions with the atoms as a whole. The latter occurs only at very low energies and it is beyond the aim of this work. In this work, I will explain Coulomb interactions of the projectile with the target electrons and nuclei, since at therapy energies, these processes dominate.  
The energies used in hadron therapy is between $\sim30$ and $\sim200\,MeV$ per nucleon. 
In this range, the residual range of the protons in tissue is between $1\,cm$ and approximately $35\,cm$. However, one cannot completely ignore the effects of nuclear interactions. \emph{For tissue-equivalent material, the probability that protons will undergo a nuclear interaction while traversing a path length segment of $1\,g/cm^{2}$ is of the order of $1\%$. At a depth of $20\,cm$, approximately 1 in 4 of the protons will have suffered a nuclear interaction}\footnote{Source: Clinical Proton Dosimetry - part 1, ICRU Report 59 (1998)}. 

\subsection{Multiple Coulomb Scattering in lateral direction}
\label{sec:mcs}
\begin{figure}[!t]
\centering
\includegraphics[width=\textwidth]{Images/sigmaParodi}
\caption{Examples of the values of $\sigma_{1,2}$ and $w$ in relation to the relative depth, at the low (left) and high (right) beam energy. From Parodi (et al.) \cite{par:latspr}}
\label{fig:sigPar}
\end{figure}
\begin{figure}[!t]
\includegraphics[width=.9\textwidth]{Images/latSig}
\caption{Example of the spread of proton and $^{12}$C ion beams in the nozzle, air gap and water for a typical treatment beamline (U. Weber, GSI Darmstadt). From García-Ramos (et al.) \cite{garc:nucu}}
\label{fig:latSig1}
\end{figure}
Coulomb scattering is the deviation of the trajectory of the particles due to the electric field of the nuclei in the medium and it is the major responsible for the broadening of the beam inside matter. Protons are heavy enough to be compared with the mass of the atoms, they have much greater momentum than target nuclei, so, the outgoing angle in their trajectory is usually very small as is the energy loss, per scattering. It becomes simple to consider the mean value of the outcoming angles (supposing the energy constant) and this is the reason why, in literature, can be found many references to \emph{multiple Coulomb scattering}.
The lateral spread is described by the Molière-Theory \cite{mol:mcs}. For small angles the higher-order terms in Molière's solution can be neglected and the angular distribution can be approximated by a Gaussian function (Highland \cite{high:mcs}) with a standard deviation 
\[
\sigma_\theta [rad]= \frac{14.1\,MeV}{\beta pc}\, Z_p\, \sqrt{d/L_{rad}} \cdot \bigg[ 1 + \frac{1}{9}\,\log_{10} (d/L_{rad} )\bigg]
\]
where the absorber material is characterized by the thickness $d$ and the radiation length $L_{rad}$. The particle has charge unit $Z_p$, momentum $p$ and velocity $\beta c$.
A correction proposed by Schneider (et al.) and a comparison with Highland's method, can be found in \cite{schn:mcs}. 

In sofware using databases (i.e., algorithms that reconstruct the dose deposition considering the beam as single object, introduced in section \ref{sec:pen}), the adopted model is usually based on a double-gaussian profile. This is obtained during extensive fitting to Monte Carlo simulations. Most of this parametrizations are based on the work of Parodi (et al.) in \cite{par:latspr}, who fitted the lateral profile of Monte Carlo simulated beams and extracted an analytical form of it
\[
D(E,z_{eq},r) = n\cdot \bigg(  \frac{1-w}{2\pi\sigma_1^2}\,e^{r^2\,/\,2\sigma_1^2} + \frac{w}{2\pi\sigma_2^2}\,e^{r^2\,/\,2\sigma_2^2} \bigg)
\]
where the $w$, $\sigma_{1,2}$ and $n$ are fitting constants that depend both on the energy $E$ and the water equivalent depth $z_{eq}$. Their relation with relative depth (i.e. depth normalized to bragg peak position) is shown in figure \ref{fig:sigPar}

%\begin{figure}[!ht]
%     \subfloat[First sub-figure\label{subfig-1:dummy}]{%
%       \includegraphics[width=.5\textwidth]{Images/latSig}
%     }
%     \hfill
%     \subfloat[First sub-figure\label{subfig-2:dummy}]{%
%       \includegraphics[width=.5\textwidth]{Images/sigmaParodi}
%     }
%     \caption{Dummy figure}
%     \label{fig:dummy}
%   \end{figure}
Between machine and patient, in air, all the scattering effects become smaller and the main responsible of beam broadening is the natural divergence of the beam, leading to a linear treand, as can be seen in figure \ref{fig:latSig1}.
But usually in dose calculation softwares, both are taken in account on the base of tabulated data. Inside matter, instead, the dominant effect is Coulomb scattering, so the broadening becomes faster expecially as the particle loses energy.

\begin{figure}[!ht]
{\includegraphics[width=\textwidth]{Images/BBp}}
\caption{Bethe Bloch formula for a proton. From Kraan \cite{Kra:range}}
\label{fig:BB}
\end{figure}

\subsection{Bethe Bloch equation}
The first version of the Bethe formula was proposed in 1932. It describes the mean energy loss from a relativistic charged particle traversing matter and at the beginning used to account only the few known effects, and contained only for the term depending on $z^2$ because was applied only in low energy cases.
Thanks to the corrections made by Barkas, Anderson and Bloch, we have now the so-called Bethe-Bloch formula
\[
-\bigg\langle\frac{dE}{dx}\bigg\rangle= \frac{4\pi}{m_ec^2}\frac{nz^2}{\beta^2}\bigg(\frac{e^2}{4\pi\epsilon_0} \bigg)^2\bigg[\ln{\bigg(\frac{2m_ec^2\beta^2}{I\cdot(1-\beta^2)}\bigg)}-\beta^2 -\frac{\delta}{2}\bigg]
\]
Where $\beta c$ is the velocity of the particle, $I=(10\,eV)\cdot Z$ is the mean excitation potential and 
\[
n = \frac{N_AZ\rho}{AM_u}
\]
is given by constants depending on the characteristic of the medium. $N_A$ is the Avogadro number, $A$ and $Z$ are the mass number and the atomic number of the medium, $\rho$ its density and $M_u$ is the atomic mass unit. Then, $\delta$ is a parameter describing the shielding of the electric field of the particle due to the polarization of the medium (density effect).
In figure \ref{fig:BB}, we show a plot of the Bethe-Bloch curve as a function of $\beta\gamma$. The very first notable thing is the minimum of this function, called the minimum ionization point. Its position is always around $\beta\gamma=3$ and it is almost independent from both the type of particle and the medium. At higher energies, the contribution of the logarithm in the formula becomes stronger and the curve rises again.

\begin{figure}[!ht]
     \subfloat[Bragg curve simulated for $\alpha$ particle in air. From \url{https://en.wikipedia.org/wiki/Bragg_peak}\label{fig:bragg}]{%
       \includegraphics[width=.47\textwidth]{Images/bragg}
     }
     \hfill
\hspace{2mm}
     \subfloat[Example of a Bragg curve of $330\,MeV/u$ $^{12}$C ions in water and calculated contributions of primary ions, secondary and tertiary nuclear fragments. From H\"attner (et al.) \cite{haet:frag}\label{fig:tail}]{%
       \includegraphics[width=.48\textwidth]{Images/tail}
     }
\caption{Bragg peak examples}
   \end{figure}
The Bethe-Bloch formula works approximately only for $\beta\gamma\ge 0.01$, taking into account all the different effects which the flying particle is subject to. The energy range of beams in proton therapy is between $\sim30$ and $\sim200\,MeV$. This imply an initial factor $0.25<\beta\gamma<0.6$ for both protons and ions. So, our attention is focused on the first part of the curve where there is a behaviour proportional to $1/\beta^2$, at the left of the maximum. This tells us that the particles lose more energy as the become slower. That determines the shape of the bragg curve where the peak of energy loss is at the end of the particle path.
Bortfeld, in \cite{bort:bragg}, shows a semi-empirical method to estimate the range of a proton in water. He states that range in water, in relation with initial energy can be calculated as
\[
R_0 = \alpha E_0^p
\]
where $E_0$ is the initial energy and $\alpha$,$p$ are fitting parameters. Obviously, this relation works only in a certain energy range which in the article is stated to be between $10$ and $250\,MeV$, coherent with our energy range.

While photons fluence decreases exponentially with distance, meaning a larger dose deposition near the surface of the target, protons have a completely different dose deposition function in relation with depth.
In figure \ref{fig:bragg}, we can see the relation between energy loss and path length, or Bragg curve. As we already said, the peak is in the last part of the path. We can notice the advantage of using protons instead of photons. In the cases of tumors located far from the surface we are allowed to hit the target with less damages to healthy tissues and we are not forced to shoot the particles from many angles, as is usually done in radiotherapy.
Even if there are good approximations as the one that Bortfeld suggests, Bragg curve has not a proper analytical function\footnote{Another approximation can be obtained by a sum of $n$ gaussian with $n\geq 10$ with minimum error, see Bangert (et al.) \cite{bang:apm}} due to the irregular behaviour of Bethe-Bloch at low energies. The Bragg peak is the reason why hadron therapy is faster (because need less entrance ports, so less time is spent for turning the machine around the patient) and less dangerous for patients than photons radiotherapy. 
In figure \ref{fig:sobp}, in the next chapter, we have a superposition of a sum of Bragg peaks and an exponential energy loss by a photon. Later in this work, I will introduce it.

\subsection{Ionization}
\begin{wrapfigure}{l}{.5\textwidth}
\vspace{-6mm}
\centering
{\includegraphics[width=.5\textwidth]{Images/survival}}
\caption{CHO cells Survival ratio realted with sose, for $X$-rays and $^{12}$C ions. From Weyrather (et al.) \cite{weyr:rbe}}
\label{fig:surv}
\vspace{-10mm}
\end{wrapfigure}
In addition to the interactions with nuclei fields, incoming protons interact with atoms, transmitting some of their energy to the surrounding electrons. If that energy is sufficient, the atom is ionized and electron is free to move inside the medium, consuming his remaining energy through Coulomb interactions and continuously slow down just like heavier charged particles. The secondary electrons momentum is determined by the energy transfered by the incoming particle so, they can have a wide range of energies. The most energetic ones can also cause further secondary ionization. 
%\begin{figure}[!h]
%     \subfloat[DNA possible deformations and breaks\label{fig:dna}]{%
%       \includegraphics[width=.57\textwidth]{Images/DNA}
%     }
%     \hfill
%     \subfloat[CHO cells Survival ratio realted with sose, for $X$-rays and $^{12}$C ions. From Weyrather (et al.) \cite{weyr:rbe}\label{fig:surv}]{%
%       \includegraphics[width=.41\textwidth]{Images/survival}
%     }
%\caption{DNA structure and survival ratio}
%   \end{figure}
Such electrons are usually referred as $\delta$-rays \footnote{Antiquated definition due to hystorical reasons. At the beginning, $\delta$-rays were not recognised as electrons} or knock-on electrons.
Ionization is an effect that causes the death (meant as the complete loss of the proliferation capacity) of the cells during radiation therapy and the main reason is DNA fragmentation. The analytical form of the \emph{survival ratio} can be written as
\[
S = e^{-\alpha D + \beta D^2}
\]
where $D$ is dose and $\alpha$ in $[Gy^{-1}]$, $\beta$ in $[Gy^{-2}]$ are radiosensitivity parameters.
Their results and the characteristic slope of this function is in figure \ref{fig:surv}.
\begin{figure}[!t]
\includegraphics[width=\textwidth]{Images/rbe12c}
\caption{Example of Radio-Biological Effect against Physical Dose, for $^{12}$C}
\label{fig:rbe12c}
\end{figure}
The ratio $\alpha/\beta$ is the discriminator factor of the capacity of the cells to repair themselves and has typical values of $1-3\, Gy$ for cells with high repair potential and close to $10\, Gy$ for repair-deficient cells.
As a measure for the effectiveness the factor RBE (\emph{Relative Biological Effectiveness}) was introduced as the ratio between $X$-ray dose and ion dose which are required to produce the same effect
\[
RBE = \frac{D_X}{D_{ion}}\Bigg|_{isoeffect}
\]
As can be seen from figure \ref{fig:surv}, at $10\%$ survival level, the RBE values for $^{12}$C ions increase from $1.6$ at $266\,MeV/u$ to $3.7$ at $11\,MeV/u$ and decrease again to $2.1$ in the $2.4\,MeV/u$ case. The reason is rather simple. At low energies, dose deposition is higher than at high energies, this is better explained in section \ref{sec:sobp}.

Surprisingly, at still lower ion energies RBE does not further increase but 
\begin{wrapfigure}{r}{.5\textwidth}
\centering
{\includegraphics[width=.5\textwidth]{Images/secondary}}
\caption{Monte-Carlo simulations showing individual tracks of $\delta$-electrons produced by energetic protons and $^{12}C$ ions penetrating tissue. From Kr\"amer \cite{Kram:track}}
\label{fig:secondary}
\end{wrapfigure}
\noindent decreases again. This can be explained by two different effects: if there is a surplus of deposited energy by a single ion, e.g. the dose is larger than the one needed to kill the cell, that surplus is wasted and that leads to saturation; another effect is that, at very low energy, the fluence become small and there is a possiblity of not depositing dose in the cell, at all. This again reduces the effectiveness.
Ion beams have a larger biological effectiveness that can be explained studying the structure of the particle track and interaction of second ionization with the molecules of DNA. 
As discussed above, ions are very energetic and their interactions are ruled mostly by collisions with the atomic nuclei. Because of their very large mass compared with the electrons, the ions are pratically moving on straight trajectories inside the medium.
Secondary electrons are mainly emitted in forward direction.
Due to collision kinematic, only a small percentage is emitted with larger angles with respect to the direction of flight of the ion and have lower energies and short ranges. The Dose around particle ``tracks'' decreases fast ($\sim1/r^2$) with the radial distance $r$. In figure \ref{fig:secondary}, we can see an image, imported  from Kr\"amer \cite{Kram:track}, showing the secondary particle production for protons and Carbon ions.
Concluding this section, we can say that RBE is an important value for heavy ion therapy but not for protons whose differences between dose and RBE $\times$ dose (usually referred as \emph{effective dose}) are neglectable.
\subsection{Fragmentation}
So far, I talked about electromagnetic interaction between particles in matter. I avoided nuclear reaction because they are of secondary importance for protons but this is not true for heavy ions.
\begin{figure}[!t]
{\includegraphics[width=\textwidth]{Images/frag}}
\caption{Illustration of the Abrasion-ablation model}
\label{fig:frag}
\end{figure}
In heavy ion therapy, beams with energies of few hundred $MeV/u$ are required. In this energy range, an ion scattering with a stationary nucleus, interacts mostly with peripheral collision with the possibility if loosing nucleons.
An example of this process is
\[
Pr_1\,+\,N_1 \, \rightarrow\, Pr_2 \,+\, N_2 + X
\]
where $Pr_i$ are the projectiles, $N_i$ are the nuclei and $X$ includes all the fragments that have been lost by incoming and target nuclei. As consequence, $Pr_2$ and $N_2$ are new lighter nuclei.
This is the so-called abrasion-ablation model that according to \cite{serb:nucRea}, as illustrated in figure \ref{fig:frag}, is the most used method for describing this effect.
García-Ramos (et al.) in \cite{garc:nucu}, say: \emph{The total reaction cross sections at high energies ($>100\,MeV/u$) can be well described by semi-empirical geometrical models and are almost constant over a wide energy range. Typical values (for water target) are about $350\, mb$ for $200\,MeV$ protons and $1400\,mb$ for $380\,MeV/u$ $^{12}$C ions. These values correspond to mean free path lengths in water of about 85 cm for protons and 21 cm for 12C ions. This means that e.g. at a depth of $10\,cm$ in water about $11\%$ of the initial proton flux was lost by nuclear reactions, while this number is much higher ($38\%$) for $^{12}$C ions.
The projectile-fragments continue travelling with nearly the same velocity and direction. These nuclear reactions lead to an attenuation of the primary beam flux and a build-up of lower-Z fragments with increasing penetration depth}.
As a consequence there is a change in the shape of the Bragg curve. Since the energy deposition depends on the charge of the particle squared $z^2$, small fragments have a longer range than the parent ions. This is the reason of the tail beyond the Bragg peak, in figure \ref{fig:tail}. 
In the first part of the tail, the contributions are given mostly by fragments like $B$, $Be$ and $Li$-ions, and then by alphas and protons, in the last part of the tail.
Obviously, the effect of fragmentation becomes more important as the range of the ions grows due to the increasing production of fragments.

%%% serve una frase conclusiva

\section{Particle treatments in clinical context}
Before the patient enters the treatment room, where the real treatment takes place, there are several steps of diagnostics and verification, necessary for the patient health and for the success of the operation.
These steps are:
\begin{itemize}
\item Diagnostic (CT, MRI, PET)
\item Treatment planning:
	\begin{enumerate}
	\item find the most suitable beam entrance ports
	\item transformation of patient CT-data to water-equivalent path-length of ions or translation material composition (for MC)
	\item calculation and optimization of the dose deposition based on the patient anatomy
	\item (for heavy ions) biological optimization and RBE calculation
	\end{enumerate}
\item Eventual further experimental verifications (for example in a water-phantom)
\item Patient positioning in gantry-room and further verification through $X$-rays
\item Irradiation
\end{itemize}
The first step of treatment planning for any radiation therapy modality is to define and delineate the target volume on the basis of modern imaging techniques. $X$-ray Comupted tomography (CT) provides quantitative information about the anatomical structures. It does it reconstructing the attenuation of photons inside target and has typical pixel resolution of $1\,mm$ in lateral direction and 2-5mm in axial direction. The CT of the patient is absolutely required in order to calculate the particle range inside the patient. Other diagnosis are often coupled with CT, as the Magnetic resonance imaging (MRI) and Positron-Emission-Tomography (PET), helping a better definition and delineation of the tumor volume inside the patient.
All this analysis must be done with all the eventual passive elements (e.g. head mask, in passive beam delivery systems, further explained in section \ref{activeScanning}) and usually are repeated right before the irradiation.
Step 2 is a core passage of the Dose calculation because it gives the patient anatomy in the format used in calculation of the dose deposition and the exact position of the Bragg peak in heterogeneous tissue.
But in order to do that, the CT gray-scale-values (given in Hounsfield units) should be converted in a number indicating the distance that a particle would cover in average in a similar medium or its stopping power.

There are many sources of uncertainty in all these steps, as the aforesaid resolution of Diagnostic. The goal of particle therapy hardware is to reach at least the same resolution. In fact, energy and direction of the beam, representing Bragg peak position (in ideal conditions), are very accurate at least on the lateral steering.
Patient positioning errors are mostly compensated by the last verifications, in loco.
%But there is still a problem of beam divergence that force the standard deviation of the profile of the beam to be several millimetres large also at small energies. [\textbf{numerical values}].
State of the art dose calculation softwares should at least keep the error smaller than the resolution in diagnostic. This is possible with Monte Carlo based softwares and it is under investigation and study for pencil beam algorithms. The reasons why, will be processed accurately in the next chapter.

%%% non so che scrivere, non mi piace


\section{Motivation and scope of this work} 
Nowadays in radiation onchology, the main challenges are improvements in accuracy and safety. 
The current compromise is a standard workflow based on setting up a single plan before the start of the therapy and then apply that plan to all the following sessions until the end of the treatment.
Scientists are working on alternative workflows where the treatment is adapted everyday (or even on the fly, while the patient is on the couch), this is called adaptive radiotherapy.
%Because of the long time lost in this routine, biggest world centers can treat, in average, only few hundred people per year. And poor accuracy means more risks for the patient.
A key role is played by softwares, used either for diagnostic, planning and verification.
The part on which I focused my work on is the treatment planning. In fact, I will investigate the most common methods used for particle dose calculation and I will implement one, Fine Sampling Beam algorithm, in an already existent software, MatRad. 
Using Monte Carlo simulated Dose and HIT fullfield experimental data as reference models, I will compare mine with already existent softwares, with the goal of overcoming accuracy of pencil beam algorithm, keeping short the computation time.  
%Furthermore

Providing scientists and doctors with strongest instruments to plan patient treatments precisely in the shortest possible time, will make Particle Radiotherapy faster, more accessible and cheaper.









\chapter{Material and Methods} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Intensity Modulated Particle Therapy}
% The Intensity Modulated Therapy is a kind of treatment that uses some bunches of charged particles or photons, whose intensity is modulated in order to release the largest part of their energy in a specific target area (i.e. the tumor volume).

Particle beams provided by cyclotron or synchrotron accelerators are typically narrow, pencil-like beams centred at the axis of the beam tube. An 
\begin{wrapfigure}{l}{.5\textwidth}
{\includegraphics[width=0.5\textwidth]{Images/Alderson}}
\caption{Simulation of dose deposition in human head. Calculated with MatRad. Dose in $Gy$}
\label{fig:alderson}
\vspace{-10mm}
\end{wrapfigure}
\noindent important task, which is performed by the so-called beam delivery system, is to distribute the beam over the planning target volume (PTV) accurately and with the desired dose distribution.
Two different basic strategies are used in clinical facilities which in their extreme forms are represented by the fully passive systems with fixed beam modulation or the fully active beam scanning techniques.


\subsection{Passive systems}
In the first case, the particle beam is adapted in three dimensions to the target volume only by passive field shaping elements, that are non-variable.
The principle of a fully passive system is shown in figure \ref{fig:passive}. The initial beam incoming from the accelerator is first broadened by a scattering device; this is needed in order to generate a flat transversal profile for a more efficient treatment. This is done by a double-scattering system.
Using a range modulator, the Bragg peak is spread out, in order to cover the entire length of the target volume. The whole spread-out Bragg peak (SOBP) can be shifted in beam direction by absorber plates (range shifter). 
The following two devices are patient specific and need to be precisely fabricated (increasing the costs and the time consumption): the collimator cuts out the field area defined by the largest target contour as seen in beam’s eye view, preventing particles outside the field to pass through. 
The range compensator adjusts the distal depth pattern, taking into account also the complex tissue composition. A major limitation of the fully passive modulation system is the fixed width of the SOBP, which may result in significant dose deposition outside the target volume, e.g. in the proximal part when the particle range is adjusted to the distal contours (in figure \ref{fig:passive}).
\begin{figure}[t]
{\includegraphics[width=\textwidth]{Images/passive}}
\caption{Example of complete Passive system. From Chu (et al.) \cite{chu:pass}}
\label{fig:passive}
\end{figure}

\section{Active Scanning Techniques}
\label{activeScanning}
In the 90s, a new technique for beam delivery was developed both at PSI (Switzerland) and at GSI (Germany). The so-called \emph{spot scanning system} at PSI and the \emph{raster scanning system} at GSI are examples of  fully active techniques.That means that no passive elements are placed between the beam deliverer and the patient, adapting the dose deposition to the target volume. The basic principle of the raster scanning system is shown in figure \ref{fig:active}.
In contrast to the passive systems there are not scattering devices. The pencil-beam is shifted in horizontal and vertical direction by a pair of deflection magnets that allow the scanning. The target volume is divided in slices and cylinders; each one corresponds to a constant beam energy and each cylinder represents a direction of flight of the particles.
\begin{figure}[!b]
{\includegraphics[width=\textwidth]{Images/active}}
\caption{Principle of the intensity-controlled raster scanning system at GSI \cite{hab:scan}}
\label{fig:active}
\end{figure}
The cylinders central axis is parallel to the beam and their section is of the same order of the dimension of the bragg peak. The volume intersection between a cylinder and a slice is called voxel.
So, at the beginning, an energy is selected and the dose is delivered to the first voxel.
When the dose limit is reached, in one voxel, the beam is shifted to the next voxel, without changing the energy. After completion of one slice extraction of particle from the synchrotron is instantly interrupted. Then the energy should be increased or decreased for the next slice and then should be delivered with the next synchrotron pulse. The scanning control system is linked with an accelerator control system that overviews all the treatment and decides the appropriate parameters of the beam for each slice irradiation during the execution of the treatment plan. 
With this system it is possible to adapt the dose distribution to any complex shape of the target volume, individually for each patient and without any patient-specific hardware.

Compared to the passive scattering techniques, in active scannig, several (up to tens of thousands) narrow proton beams are used for irradiating a target volume. Each beam energy and dose deposition is weighted in a computer-based optimisation procedure in order to find the best solution for the prescribed dose in the target volume and for safeguarding and protect the critical organs as much as possible.
This is precisely what a treatment planning software does.


\subsection{Spread Out Bragg Peak}
\label{sec:sobp}

A mono-energetic proton beam, by itself, is non suitable for cancer treatment in a large volume because of its longitudinally narrow Bragg peak. 
It is necessary to \emph{spread out} the Bragg peak providing a uniform dose deposition inside the target volume. As shown in figure  \ref{fig:sobp}, the flat-top SOBP function has been obtained summing the contribution of many single bragg peaks. Weighting the energy distribution of the incoming beam, this can be easily done by 
\begin{wrapfigure}{r}{.5\textwidth}
{\includegraphics[width=0.5\textwidth]{Images/SOBP3}}
\caption{Comparison between protons Spread Out Bragg Peak and photons energy loss in matter. From Degiovanni \cite{deg:lin}}
\label{fig:sobp}
\vspace{-10mm}
\end{wrapfigure}
\noindent a dose calculation software.
By varying the energy and the intensity of the monochromatic beam without changing the flight direction, we can overlap the effect of every bunch in the same cylindric volume inside the target and, summing their contribution with the correct weights, we are able to obtain a constant value of dose deposited in the whole target area.

Proton beams has been studied by various investigators, in particular by Bortfeld and Schlegel in \cite{bort:SOBP}. They have developed a simple analytical way for determining the weights of proton beams with various initial energies required to create an SOBP for a proton beam. Jette and Chen (\cite{jett:SOBP}) improved this method, finding a more precise one. 

\subsection{Hounsfield Unit and Stopping Power}
Delivering the correct dose during clinical trials, in proton treatment planning systems (TPS), requires the appropriate conversion from Hounsfield unit (HU) to relative linear stopping power (RLSP). In contrast to the photon dose calculation that depends on the electron density of a material, proton radiation therapy  needs the stopping power of a certain material in order to calculate the energy loss, to describe the interactions in matter and
\begin{wrapfigure}{l}{.6\textwidth}
{\includegraphics[width=0.6\textwidth]{Images/dpp}}
\caption{PDD example. Grant (et al.) \cite{gra:hu}}
\label{fig:pdd}
\end{wrapfigure}
\noindent to evaluate dose for protons. So, the CT calibration curve converts Hounsfield unit values to relative stopping power, in a proton treatment planning system.
Hounsfield units (HU) were introduced by Sir Godfrey Hounsfield and are used in order to have a standardised form to indicate CT numbers.
They depend on the measured linear attenuation coefficient $\mu$ of the medium.
In formula,
\[
HU=1000\,\frac {\mu -\mu _{water}}{\mu _{water}-\mu _{air}}
\]
where $\mu _{water}$ and $\mu _{air}$ are the linear attenuation coefficients of water and air, respectively.
So, the radiodensity of distilled water at standard temperature and pressure is defined as $0\,HU$; instead the radiodensity of air in the same condition is $-1000\,HU$
The relative linear stopping power of a certain material is calculated starting from computation of the percent depth dose (PDD) curve and the relative shift with the PDD of water for the same beam (example in figure \ref{fig:pdd}).
So
\[
RLSP = \frac{\Delta x}{d_m}
\]
where $\Delta x$ is the change in the depth of the distal $80\%$ PDD when the tissue-substitute material is introduced into the beam, and $d_m$ is the thickness of the material.
In figure \ref{fig:HU}, Grant (et al.) in \cite{gra:hu}, fitted data obtained for several materials and obtained a (almost linear) relation between Hounsfield unit and relative linear stopping power.

This relation is fundamental to convert the CT voxel value in a water-equivalent stopping power, in order to be able to calculate radiation depth.
\begin{figure}[t]
\centering
{\includegraphics[width=.9\textwidth]{Images/hu_rlsp}}
\caption{RLSP and HU relation. From Grant (et al.) in \cite{gra:hu}}
\label{fig:HU}
\end{figure}



\section{Dose Calculation}
Dose Calculation is the very core of the treatment planning. 
Nowadays there are three main ways to evaluate dose deposition inside matter. These methods can be divided in two categories: the one that simulates the path of the single particle such as Monte Carlo simulations and the methods simulating the dose deposition of the whole bunch of particles at once, such as Analitical Probabilistic Modelling (APM) and the pencil beam models.


%% add  something 

%solo questo????

\subsection{Monte Carlo}
The Monte Carlo simulations work on the basic idea of reconstructing the path of all the single particles inside a medium. The software calculates the probability of all the possible interactions between particle and matter and chooses randomly, on the base of these probabilities, which effects the single particle undergoes and, for example, length and direction of its free path between two interactions.

In order to speed up this method, the modern software simulates only a part of the particles and assembles a certain number of successive scattering in a single and more complex one. Although Monte Carlo simulations are highly expensive in terms of time, they ensure very accurate simulations. This is the reason why we choose it as our ``gold standard''.
The Monte Carlo simulations used in this project were produced by \emph{FLUKA}\footnote{More info about Fluka at \url{http://www.fluka.org/fluka.php}}. As supporters say on their website: \emph{it is able to simulate with high accuracy the interaction and the propagation in matter of about 60 different particles. it is extensively used at CERN for all the beam-machine interactions, the radio protection calculations and the facility design of some forthcoming projects. 
Outside CERN, among various applications worldwide, FLUKA serves as a core tool for the HIT and CNAO hadron-therapy facilities in Europe and it is supported also by \emph{INFN}.}

In particular, the software we used does not randomly choose the interaction on the base of its probability and then establishes the free path of the particle before that interaction, but it does the opposite. Before, it randomly determines the free walk of the particle, from its starting point, for every possible interaction, separately. %(for example, it establishes that the particle flies for a distance $x_1$ before interac.
Then, it chooses the process that occurs earlier.
For example, let us consider a particle that starts from a point $x_0$; after establishing all the $n$ possible interaction processes that the particle can undergo, the software simulates the walk of the particle $n$ times, from $x_0$ to the $x_i$ (point where the i-th interaction process happens). At the end, it takes $\min(\{ x_i-x_0 \})$ as the free path of the particle and the i-th interaction as the occurring one.

%Add something..... %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%da riscrivere ultima parte. scrivere i nomi delle sigle o aggiungi le sigle quando citi i nomi precedentemente. questo anche nella parte successiva,

\subsection{Pencil Beam} 
\label{sec:pen}
A standard dose calculation software is divided into two main parts: dose influence matrix calculation and optimization. The second one, is the process used to determine the weights of the charged particle fluence distribution according to the user constraints. These include CT, plan and segmentation useful to define the direction of the beam and to prescribe the limits of the dose deposition for every area.
Mathematically, the dose $d_i$ in the voxel $i$ is computed as
\[
d_i = \sum_j D_{ij}w_j\,\,\,\,\,\,\textit{or:}\,\,\,\,\,\vec{d} = D\vec{w}
\]
where $D$ is the dose influence matrix and $\vec{w}$ is a vector containing the weights of the pencil beams; $i$ and $j$ are the indeces of the voxel and the pencil beam respectively.
I will focus on methods for the calculation of the dose influence matrix.
The Pencil beam algorithms are based on a method called \emph{Ray Tracing}\footnote{On Schaffner (et al.) \cite{schaf:pba}, there is a comparison between ray casting algorithm, other methods and Monte Carlo simulations}. This method consists in calculating the energy loss of the particles along a straight line inside the CT of the patient. For doing this, the method calculates the path of straight lines being parallel to the beam. For each three-dimensional pixel, i.e. \emph{voxel}, it is possible to calculate the \emph{water equivalent distance} (otherwise referred as $z_{eq}$ or radiation length) based on the distance traveled by the particles and the stopping power of that voxel, i.e., the distance that a particle should travel in order to undergo the same attenuation as in the voxel. The calculation of $z_{eq}$ is necessary because the algorithm works with the help of pre-computed Monte Carlo simulations considering particles at all the possible energies that can be provided by the accelerator, in a water environment. 
For a precise voxel, the water equivalent distance is the sum of the products between the path inside the $i$-th voxel $l_i$ and its water equivalent density\footnote{To better explain what is this, $\rho_i$ is the distance that a particle would travel inside the voxel, if it was in water, divided by the actual travelled distance. At parity of energy loss} $\rho_i$, over all the voxels $i$ crossed before that particular voxel.
In formula
\[
z_{eq} = \sum_{i} \rho_i\cdot l_i
\]
Obviously, it is a value that strictly increases with depth.
In figure \ref{fig:rd}, there is an example for a CT of a phantom with human skull and artificial tissue.
\begin{figure}[!t]
\centering
\includegraphics[width=.45\textwidth]{Images/ald_rx}
\includegraphics[width=.45\textwidth]{Images/ald_rd}
\caption{Slice of the radiation lenght matrix (right) of a pseudo-human head CT (left). The beam comes for the upper part in vertical direction}
\label{fig:rd}
\end{figure}
After the evaluation of the radiation depth, the second step is to calculate the lateral distance  $r$ of every voxel from the beam central axis. This also allows to highlight the maximum volume covered by the radiation and restrict the operations to that area to save computation time. 
Then, the information provided by these two steps is used in the following part of the software in order to calculate the dose. Therefore, this part represents the core of the software. By knowing the energy of the beam, the software finds the corresponding pre-computed Bragg peak stored in the look-up tables, whose function is $B(z)$, depending on depth $z$. Then, it adapts the Bragg peak in both longitudinal and lateral direction, depending on the morphology of the target. 
Formally, the calculated dose for the single bixel $d_{sb}$ is 	
\[
d_{sb} = B(z_{eq}) \cdot L(z_{eq},r)
\]
where $L(z_{eq},r)$ is the lateral spread of the beam estimated with the double gaussian method explained in section \ref{sec:mcs}, depending also on the radial coordinate $r$. It has a shape like
\[
L(z_{eq},r) = \frac{1}{\sqrt{2\pi}\,\sigma_n}e^{r^2/2\sigma_n^2} + \frac{1}{\sqrt{2\pi}\,\sigma_b}e^{r^2/2\sigma_b^2} 
\]
where $\sigma_n$ and $\sigma_b$ are the standard deviation of the narrow and broad parts of the double gaussian function and both depend on $z_{eq}$
The full process is repeated for every \emph{bixel}, i.e. for every beam energy and for every direction. In order to get the final dose, the software makes a loop over all the bixels whose steps have the aim to calculate the corresponding value of the dose and store it in a sparse matrix. The final dose influence matrix is then the result of the sequential sum of the doses calculated for every bixel.
The figure \ref{tikz:pba} shows a flowchart reassuming the main steps of the algorithm.

A standard pencil beam algorithm does not take into account the possibility for the protons to change direction of flight inside the target. So, the algorithm rescales the range of penetration based only on the stopping power along a straight line. In some cases, this leads to steep dose gradients. This issue and others will be treated in chapter \ref{chap:res}.


\subsubsection{Syngo and MatRad}
Syngo RT Planning is a professional software produced by Siemens that is based on Pencil beam model. Syngo is used in many radiotherapy centers such as HIT in Heidelberg and has been used in this work for being compared with matRad\footnote{Software and more information can be found at: \url{http://e0404.github.io/matRad/}}, an open source dose calculation and optimization toolkit for photon, proton and carbon ion therapy. It is maintained by the department of Medical Physics in Radiation Oncology at the German Cancer Research Center DKFZ. 
These are the fastest and the most robust software we had access to, but they have limits in the precision. In fact, they are affected by difficulty of accurately describing the dose around high gradient zones, especially near bones and air bubbles.

\newpage
\begin{figure}[!h]
\centering
\begin{tikzpicture}[node distance=1.5cm, xshift=5cm]

\node (start) [startstop, xshift=-5cm] {Import data and Plan};

\node (rt) [process, below of=start] {Ray Tracing};
\node (geo) [process, below of=rt, yshift=-0.5cm] {Calculate lateral distances};
\node (calcdose) [process, below of=geo, yshift=-0.5cm] {Calculate Dose};
\node (store) [process, below of=calcdose] {Sum Dose};

\node (loopbixel) [decision, below of=store, yshift=-1cm] {For each Bixel};
\node (loopray) [decision, below of=loopbixel, yshift=-1.8cm] {For each Ray};

\node (end) [startstop, below of=loopray, yshift=-1cm] {Export dose influence matrix};

\node[coordinate] (r_store) [right of=store, xshift=2cm] {};
\node[coordinate] (r_loopbixel)   [right of=loopbixel, xshift=2cm] {};
\node[coordinate] (bef_dose) [below of=geo, yshift=0.5cm] {};
\node[coordinate] (r_bef_dose) [right of=bef_dose, xshift=2cm] {};
\node[coordinate] (bef_calcdose) [below of=rt, yshift=0.5cm] {};
\node[coordinate] (r_bef_calcdose) [right of=bef_calcdose, xshift=3cm] {};
\node[coordinate] (r_loopray)   [right of=loopray, xshift=3cm] {};

\node[coordinate] (r_phantom) [right of=store, xshift=5cm] {};
\node[coordinate] (l_phantom) [left of=store, xshift=-5cm] {};


\draw [arrow] (start) -- (rt);
\draw [arrow] (rt) -- (geo);
\draw [arrow] (geo) -- (calcdose);
\draw [arrow] (calcdose) -- (store);
\draw [arrow] (store) -- (loopbixel);
\draw [arrow] (loopbixel) -- (loopray);
\draw [arrow] (loopray) -- (end);

\draw[arrow] (loopbixel) -- (r_loopbixel) -- node[midway, left]{}  (r_bef_dose) -- (bef_dose) ;
\draw[arrow] (loopray) -- (r_loopray) -- node[midway, left]{}  (r_bef_calcdose) -- (bef_calcdose) ;

\end{tikzpicture}
\caption{Pencil beam algorithm flowchart. This is repeated for every direction of irradiation}
\label{tikz:pba}
\end{figure}
\newpage

\subsection{Pencil beam fine sampling}
A modified version of the pencil beam algorithms consists in the \emph{Fine Sampling}\footnote{Soukup (et al.) \cite{souk:pba}, describes the overall methods behind the Fine Sampling Pencil Beam and shows some of the results}.
The main difference is that fine sampling, as the name suggests, does not calculate the dose for the whole bixel in one step but divides the bixel in smaller sub-beams and repeats the dose calculation for all of them. 
\begin{figure}[t]
{\includegraphics[width=.5\textwidth]{Images/FurtherSigmaAnalysis2_01}}
{\includegraphics[width=.5\textwidth]{Images/FurtherSigmaAnalysis2_02}}
\caption{$\gamma$-index value (compared with MC) and maximum percentage error, variating $\sigma_{sub}$, for the inhomogeneous phantom. The red line means the gamma index value of matRad simulation.}
\label{fig:sigsub}
\end{figure}

The first part of the code is very similar. But before running the loop over the bixels, we need to assign a weight to the sub-components. These are arranged on a square grid at regular distance one from another, covering $3\,\sigma$ of the lateral spread of the beam profile. 
The gaussian profile of the beam (with standard deviation $\sigma$) and the profile given by the sum of the components (with standard deviation $\sigma_{sub}$) should be equal
\[
\frac{1}{\sqrt{2 \pi}\,\sigma}e^{(x^2+y^2)/2\sigma^2} = \sum_{i=1}^N \frac{w_i}{2 \pi\sigma_{sub}}e^{[(x-\mu_{xi})^2+(y-\mu_{yi})^2]/2\sigma_{sub}^2}
\]
Where $N$ is the number of sub-components, $\mu_{xi}$ and $\mu_{yi}$ are the grid point and $w_i$ are the weights of each component.
A component near to the center should give a higher contribution than a component further away and they do not have the same impact on the dose. So I imposed the weights to be gaussian shaped and I used a function that finds the correct weights, minimizing the square of the difference between the aforesaid functions.
In formula
\[
\sum^M_{j=1}\, \bigg|\frac{1}{2\pi\sigma}e^{(x_j^2+y_j^2)/2\sigma^2}\, - \sum^N_{i=1} \,\frac{W}{2\pi\Sigma}e^{(x_j^2+y_j^2)/2\Sigma^2} \,\frac{1}{2\pi\sigma_{sub}}e^{[(x_j-\mu_{xi})^2+(y_j-\mu_{yi})^2]/2\sigma_{sub}^2} \bigg|^2
\]
Where $M$ is the number of points on which the minimization run, $W$ and $\Sigma$ are the parameters of the minimization.
I run simulations on all the configuration, with $\sigma_{sub}/\sigma$ by varying from 0.1 to 0.8. 
The value of $\sigma_{sub}$ has been chosen on the basis of the accuracy of these simulations (evaluated with $\gamma$-index test\footnote{Discussed in detail in section \ref{sec:gam}}, compared with Monte Carlo simulations) and the maximum error (on the difference between gaussian profile and sum of gaussians).
In figure \ref{fig:sigsub}, you can see the results for one of the cases. The results are very similar for every phantom and from these we obtained that the most suitable value for our purposes is $\sigma_{sub}/\sigma=0.25$ because it has the highest gamma-index between the values with an error smaller than $1\%$.
\begin{figure}[t]
{\includegraphics[width=.5\textwidth]{Images/gaugrid_2}}
{\includegraphics[width=.5\textwidth]{Images/gausurf_plus}}
\caption{Gaussian lateral profile with highlighted position of the sub-components (left). Gaussian profile of sub-components (right)}
\label{fig:gaugrid}
\end{figure}

%%% adjust sigma figures: -xlabel;  -repeat calculations;  -

%%%% da fare:
%%%%		- ricalcolare gamma index e computation time per numero di sub-components
%%%%		- ricalcolare gamma index e max percentage error per sigma
%%%%		- aggiungere grafici ottenuti
%%%%		- infornare a 180° per un'ora

%%% add sigma further analysis

The other novelty introduced into the code is a function that projects the radiation depth on the central axis of every sub-component. In other words, a new ray tracing for the smaller sub-beams.
Then the radiation depth, used for rescaling longitudinal distances, is recalculated only for the central axis of the single sub-beam (e.g., the Bragg peak is not deformed, it is only scaled on the direction of the beam). I would like to focus the attention of the reader on this point. The sub-component does not take in account the general morphology of the target but only the one corresponding to the voxels crossed by its central axis. In other words, it does not take in account inhomogeneities near the sub-beam axis if they are not \emph{on} the axis. I will show in details this issue in the next chapter. 
In order to compensate this deficiency, a large number of sub-beams should be used. 
As can be easy to forecast, the higher is the number of sub-beams the higher is the accuracy of the simulation. 
The number of sub-components used is $N=324$. This number has been chosen after analysis on computation time, more than accuracy. As one can see from figure \ref{fig:gausn}, the gain in accuracy is almost irrelevant compared with the loss in computation time. For high $N$, the computation has also other problems, such as memory size issues because of the huge amount of data in the RAM.
\begin{figure}[t]
{\includegraphics[width=.5\textwidth]{Images/gamman}}
{\includegraphics[width=.5\textwidth]{Images/timen}}
\caption{Gamma index (left) and time consumption (right) as function of $n$. The value $n$ on the $x$-axis is related with the number of compontents as $N=(2n+2)^2$}
\label{fig:gausn}
\end{figure}

The new flowchart of the model (\ref{tikz:fs}) is very similar to the previous one but with one more loop.
Apart for the weights, there is a function that allows the projection of the radiation depth of the central axis of the component on the other points of the components.

\newpage
\begin{figure}[!ht]
\centering
\begin{tikzpicture}[node distance=1.2cm, xshift=5cm]

\node (rt) [process] {Ray Tracing};
\node (start) [startstop, left of=rt, xshift=-3cm] {Import data and Plan};
\node (geo) [process, below of=rt, yshift=-0.2cm] {Calculate lateral spread};
\node (weight) [process, below of=geo, yshift=-0.2cm] {Weight Calculation};
\node (proj) [process, below of=weight, yshift=-0.2cm] {Projection};
\node (calcdose) [process, below of=proj, yshift=-0.2cm] {Calculate Dose};
\node (store) [process, below of=calcdose, yshift=-0.2cm] {Sum Dose};
\node (loopcomp) [decision, below of=store, yshift=-1.4cm] {For each Comp};
\node (loopbixel) [decision, below of=loopcomp, yshift=-2.6cm] {For each Bixel};
\node (loopray) [decision, below of=loopbixel, yshift=-2.4cm] {For each Ray};
%\node (end) [startstop, below of=loopray, yshift=-1cm] {Export Dose};

\node[coordinate] (r_loopcomp) [right of=loopcomp, xshift=2cm]{};
\node[coordinate] (bef_calcdose) [below of=proj, yshift=0.5cm] {};
\node[coordinate] (r_bef_calcdose) [right of=bef_calcdose, xshift=2cm] {};


\node[coordinate] (r_loopbixel)   [right of=loopbixel, xshift=3cm] {};
\node[coordinate] (bef_proj) [below of=weight, yshift=0.5cm] {};
\node[coordinate] (r_bef_proj) [right of=bef_proj, xshift=3cm] {};

\node[coordinate] (r_loopray)   [right of=loopray, xshift=4cm] {};
\node[coordinate] (bef_geo) [below of=rt, yshift=0.5cm] {};
\node[coordinate] (r_bef_geo) [right of=bef_geo, xshift=4cm] {};

\node[coordinate] (r_phantom) [right of=store, xshift=5cm] {};
\node[coordinate] (l_phantom) [left of=store, xshift=-5cm] {};


\draw [arrow] (start) -- (rt);
\draw [arrow] (rt) -- (geo);
\draw [arrow] (geo) -- (weight);
\draw [arrow] (weight) -- (proj);
\draw [arrow] (proj) -- (calcdose);
\draw [arrow] (calcdose) -- (store);
\draw [arrow] (store) -- (loopcomp);
\draw [arrow] (loopcomp) -- (loopbixel);
\draw [arrow] (loopbixel) -- (loopray);
%\draw [arrow] (loopray) -- (end);

\draw[arrow] (loopcomp) -- (r_loopcomp) -- node[midway, left]{} (r_bef_calcdose) --(bef_calcdose) ;
\draw[arrow] (loopbixel) -- (r_loopbixel) -- node[midway, left]{}  (r_bef_proj) -- (bef_proj) ;
\draw[arrow] (loopray) -- (r_loopray) -- node[midway, left]{}  (r_bef_geo) -- (bef_geo) ;
\end{tikzpicture}
\caption{Pencil beam algorithm flowchart}
\label{tikz:fs}
\end{figure}
\newpage




%b\footnote{Siemens link to Syngo page: \url{https://www.healthcare.siemens.it/medical-imaging-it}}

\section{Phantom Data}
\emph{Phantom}, in this field, is a technical word used to indicate a object built on purpose to run tests on real and physically well know devices. Building his own phantom, one can decide its physical properties in order to simplify errors detection, during numerical simulation. 

\subsection{Software Phantoms}
\label{sec:pha}
\begin{figure}[h]
{\includegraphics[width=\textwidth]{Images/phantoms2}}
\caption{From left ro right: Homogeneous; Inhomogeneous; Airbox; and Soukup phantom}
\label{fig:phantoms}
\end{figure}
When there is need to compare dose calculation softwares, the simpliest thing to do is to ``build'' a CT that represents a phantom with features decided on purpose by the programmer. This gives the chance to test this kind of softwares in situation that can highlight their weaknesses and their strengths.


For single beam comparison, I used four types of phantoms:
\begin{itemize}
\item The homogeneous waterphantom, a simple $80\times80\times80\,mm$ water box with relative stopping power (RSP) equal to 1.
\item The inhomogeneous phantom, a box of the same dimensions as the above one but divided in halves, one half contains water and the other one contains a softer material of $0.8$ RSP. The cut is parellel to the beam direction and it is right at its center.
\item The airbox phantom, a water box of the usual dimensions with a $20\times20\times20\,mm$ air box inside. The airbox was placed in different positions.
\item The Soukup phantom \footnote{This phantom was used from Soukup (et al.) in their article about pencil beam fine sampling \cite{souk:pba}}, a waterbox with two inhomogeneities of $1.5$ and $0.4$ RSP at $30\,mm$ of depth, both $20\,mm$ width.
\end{itemize}

In figure \ref{fig:phantoms} all these phantom are shown. You may notice that all the inhomogeneities are placed in order to cut the beam in halves. The beam direction is veritcal and the beam axis is at the center of the orizontal axis.


\subsection{Water Phantom}
\begin{figure}[h]
{\includegraphics[width=0.55\textwidth]{Images/PTW_31015}}
{\includegraphics[width=0.45\textwidth]{Images/PTW_31015spec}}
\caption{Single PTW-31015 ionization chamber, on the right (left picture), and precise description of its dimensions (right picture). From PTW website accessed on July 16th.}
\label{fig:31015}
\end{figure}
In this work, I compared my dataset with a set of measurements done at Heidelberg Ion-Beam Therapy Center on a phantom. The phantom used for the measurements was a plexiglass box filled with water with a detector inside. The detector is composed by 24 ionization chambers models \emph{PTW-31015}\footnote{A complete set of specifications can be found at \url{https://www.rpdinc.com/ptw-31015-003-cc-pinpoint-chamber-976.html}} produced by PTW-Freiburg\footnote{\url{http://www.ptw.de/home_start.html?&no_cache=1}}. The ionization chambers have a cylindrical shape with vented sensitive volumes of $30\,mm^3$ ($1.45\,mm$, length $5\,mm$) and $2.9\,mm$ in external diameter (figure \ref{fig:31015}).
%% The productor declares a nominal response of $800\,pC/Gy$. (\textbf{from this i should find the error on the measurement but i miss some data})
The chambers are arranged in 6 lines on 3 levels, inside a plexiglass support that allows to immerse the chambers directly into the water. In figure \ref{fig:24cham} the set up of the detector is shown.
The colleagues of the Heidelberg Ion-Beam Therapy Center collected data from measurements in nine different positions where the whole detector was moved inside the volume and across the surface of a cubic target area. More about these measurements will be reported in the next chapter.

\begin{figure}[h]
{\includegraphics[width=.5\textwidth]{Images/doublew}}
{\includegraphics[width=.5\textwidth]{Images/doublew2}}
\caption{Three-dimensional views of the \emph{Double Wedge} phantom with details of the plastic absorber (green), water (blue), target cube (red) and beam direction (black line). Scales are in tens of voxel, one voxel having a resolution of $1\times1\times1\,mm^3$}
\label{fig:doublew}
\end{figure}

\subsection{Double Wedge} 
The Double Wedges is a particular kind of phantom built by HIT colleagues with the aim of testing the effects of the treatment through a surface that has a considerable slope at the entrance surface. 
Figure \ref{fig:doublew} represents the CT of this phantom. The light blue box is the limit of the matrix and its dimensions are $512\times512\times390\,mm$. The black line is the direction of the beam and points towards $-x$ direction (or in other words, from the air side toward the water). The line passes though a point called \emph{Isocenter} and marked with a red cross. It represents the center of the axis in the machine reference system.
The actual phantom is a parallelepiped of plastic (with a known attenuation coefficient) filled with water. Its dimensions are around $500\times200\times350\,mm^3$. Along the beam direction, with comparable dimensions, there is a \emph{double wedge}, i.e. a couple of triangular plastic attenuators of $1.165$ relative stopping power. These attenuators have a gradient of $30$ and $60^\circ$, respectively.
The red square box with side of $50\,mm$ is the target where we want to release the greatest part of the dose. 

\section{Analysis techniques}
All the simulations collected during this study, should finally be compared one another. 
In fact, the key problem in this work is, to find a discriminating factor that could establish if one simulation is more or less accurate than another one. Two three-dimensional matrices can be compared in many ways, but is it possible to find a method that gives us a complete analysis and a simple way to construct it?
This is not a simple question to deal with. 

Furthermore, there is a problem in showing the result. For example, if one plots only a slide of a 3D-matrix, he could highlight some lacks and do not see others. Then we need a method that qualitatively tells us where the issue is and how much it influences the results.

In order to validate this work, I used three different kinds of comparison. In this section, I will explain them.

\subsection{Dose difference Maps}
\begin{figure}[t!]
\includegraphics[width=\textwidth]{Images/diffmap2}
\caption{Bragg peak (upper left) and dose difference maps. Axis in $cm$}
\label{fig:diff}
\end{figure}
The first and most intuitive method is to calculate the strict difference, element-by-element, between the two matrices I want to compare. Conceptually it is very simple. If we want to obtain the difference map between matrix $A$ and $B$, we define the generic element of $C$, as
\[
C_{ijk} = A_{ijk} - B_{ijk}
\]
The resulting matrix should now be plotted in a comprehensible way. In figure \ref{fig:diff}, there is a plot of a slice of a dose deposition cube (i.e. a bragg peak, in the upper left) and the difference between that cube and the same one, translated of $0.5\,mm$ towards right on the orizontal axis. 
The difference maps are plotted in different ways: the first one is a slide of the difference-dose matrix and it is plotted flat with a colormap; on the lower left, there are the same data but plotted in 3D; the last one instead is the tridimensional representation of the whole dose-difference cube, given with similar color-code.

Given the simmetry, the 3D graphs do not tell us anything new, that is why we will always show Dose difference maps using the upper-right graph.

A point to focus on is that the maximum error is over the $10\%$ of the maximum value of the dose deposition. In addition, the \emph{percentage difference}\footnote{In all this work, I will refer to percentage difference as the difference point-by-point between $A$ and $B$, divided the maximum of $A$, times 100} has an average value of $3.5\%$ (in the area where dose is above $1\%$ of the maximum). 
This seems to be a large disagreement. Instead, it is the same dose shifted only of half a millimetre, that can be considered reasonably inside our error. The result derives from the fact that we are subtracting two curves with high gradients. So, we should find a better way to account for this.

\subsection{Dose Profiles}
To have a better prospective on the comparison between high gradient curves,
\begin{wrapfigure}{l}{.5\textwidth}
\vspace{-5mm}
{\includegraphics[width=0.5\textwidth]{Images/prof}}
\caption{Profiles example}
\label{fig:prof}
\vspace{-5mm}
\end{wrapfigure}
\noindent these should look at dose profiles. 
These are the dose values taken from a line of voxels and plotted in relation to their position. For example, if I want to see the dose profile along the center of the beam, I should select the dose value of the voxels traversed by the beam axis and plot them with their corresponding position.
In figure \ref{fig:prof}, there are shown the most common examples: \emph{i)} the entrance profile, (normally) depending only by the machine properties; \emph{ii)} the peak profile, with notable gaussian shape and \emph{iii)} the central profile, usually refered to as the \emph{Depth-Dose Profile} (DDP). There is another important DDP that is used for displaying the total dose as a function of the depth: it is called \emph{Integrated-Depth-Dose profile} (IDDP). This is similar to DDP but it takes not only a line of voxels, but a whole plane (or volume) and sums all the elements on the direction perpendicular to the beam axis, obtaining the same thing as DDP but integrated on the lateral direction.

This method represents a better way to compare two high gradient curves but it is not enough. We need a discriminator that can give us a simple and fast way to judge if a simulation is more accurate than another one.


\subsection{Gamma Index}
\label{sec:gam}
The peculiar attribute of the dose deposition simulations is to have large flat or low-gradient zones, usually inside and outside the target area (tumor in real cases) and very-high-gradient zones in the region surrounding it.
The mathematical difference between two datasets will not give us a full and exhaustive comparison because it could highlight an area of significant disagreement near high-gradient regions and a more quantitative assessment may be needed for final system approval.   
We need an instrument that could give us an estimation of accuracy in dose difference and in \emph{distance} (like the Dose profile does).
In order to get this, I used the method shown in Low (et al.) \cite{low:gamma} or else the \emph{gamma-index test} that uses a distance-to-agreement (DTA) distribution to determine the acceptability of the dose calculation. 
The distance between the two nearest voxels (of the two compared matrices) that have the same dose is called DTA. Distance and dose difference are calculated for every voxel and checked if they exceed the respective tolerance limits. Points are valued on the basis of the obtained result by appling these criteria. This method provides an index for every voxel (further called $gamma$-index) that indicates agreement (or disagreement) between regions that pass (or fail) the test.
In mathematical terms, (comparing matrix $A$ with subscript $a$ and $B$ with subscript $b$) the gamma index is
\[
\gamma(r_a) = min\{\Gamma(r_a,r_b) \}\,\,\,\,\forall\{ r_b\}
\]
where
\[
\Gamma(r_a,r_b) = \sqrt{\frac{r^2(r_a,r_b)}{\Delta d^2_M} + \frac{\delta^2(r_a,r_b)}{\Delta D^2_M}}
\]
In this formulation, $r^2(r_a,r_b)$ is the distance and $\delta^2(r_a,r_b)$ is the dose difference between two point of $A$ and $B$. Instead, $\Delta d^2_M$ and $\Delta D^2_M$ are the criteria for distance and Dose, respectively.
These criteria represent the acceptance value that we want to assign to our comparison. For example: if we have a matrix with a resolution of $3\times3\times3\,mm$, a distance criteria $\Delta d^2_M = 3\,mm$ would be the optimal one. So, if the matrix $B$ has a point with the same dose value as the point $r_a$, at a distance smaller than $3\,mm$, the gamma index test is passed for that point $r_a$.
In general, the pass-fail criteria therefore become
\[
\gamma(r_a)\leq1\,\,\,\,\,\,\textrm{calculation passes}
\]
\[
\gamma(r_a)>1\,\,\,\,\,\,\textrm{calculation fails}
\]
The output of this test is usually given in percentage of points passing the test on the total number of points of the matrix.
From this result, the points with dose value smaller than $\Delta D^2_M$ criteria are excluded.
Ad the end, we finally built the instrument that we needed. Anyway, in the next chapter I will expose the results using all the aforesaid methods.



\begin{figure}[b]
{\includegraphics[width=\textwidth]{Images/Siemens24chambers}}
\caption{Photographs of the complete mounted detector from different points of view. Provided by the technitians of Heidelberg Ion-beam Therapy Center}
\label{fig:24cham}
\end{figure}




\chapter{Results} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{chap:res}

In order to give an accurate and full comparison between the algorithms that we considered, I will proceed by several steps, starting from the simplest example towards more complicated and realistic treatment fields.

I will present the results using all the methods shown in the previous chapter. In the section about elementary beams: I used proton beams at $125\,MeV$ for the first three phantoms; beam energy of $125$ and $172\,MeV$ for the Soukup phantom; and $21$, $125$ and $172\,MeV$ for the double wedge phantom.

\section{Elementary Pencil Beams}
%\begin{figure}[h!]
%\centering
%\vspace{-10mm}
%\includegraphics[width=0.8\textwidth]{Images/images_thesis/homo_dose}
%\caption{Dose deposition in water box for (from left to right) MatRad, Fine Sampling, Monte Carlo}
%\label{fig:WB1}
%\end{figure}

\begin{figure}[!b]
\centering
\subfloat[][\emph{MatRad}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_02}}\quad
\subfloat[][\emph{Fine Sampling}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_03}}\quad
\subfloat[][\emph{Monte Carlo}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_04}}\quad
\caption{Dose deposition in water box.}
\label{fig:WB1}
\vspace{-20mm}
\end{figure}
The basic starting point for a comparison between similar methods is to actually confirm that they are similar in very simple cases. Only after that, one is allowed to proceed and analyse more sophisticated problems. 
A normal treatment plan consists in thousands of elementary pencil beams (the ones calculated for every bixel).
An eventual small error in the single pencil beam, could sistematically be summed in the case of a full field simulation, resulting in a huge error.

\subsection{Water Box}
The easiest case that we can put our hands on is the full Water Box. Nothing more than a fully symmetrical cubic box made of water. With this test, we want to confirm that our method has not programming mistakes and, in general, can represent a Bragg peak, as sum of many Bragg peaks. 
The dose simulations gave us the result in figure \ref{fig:WB1}, where the Isocenter slice of the dose cubes is plotted.



The full agreement of these data sets can be confirmed by the percentage difference map and the $\gamma$-index test. Figure \ref{fig:WB1gam} shows the results.
\begin{figure}[!ht]
\centering
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_05.png}} 
\subfloat[][\emph{$\gamma$-index \\ $99.92\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_06.png}}
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_07.png}} 
\subfloat[][\emph{$\gamma$-index \\ $99.92\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_08.png}} 
\caption{Comparison between MatRad and Monte Carlo (left), Fine Sampling and Monte Carlo (right) in water box.}
\label{fig:WB1gam}
\end{figure}

A notable thing is the light shift in the Bragg peak position. From the central profile (in figure \ref{fig:WB1prof}), we measured a shift of $0.13\pm0.10\,mm$ and $0.12\pm0.10\,mm$ in the direction of the beam, for Fine Sampling and MatRad, respectively. In figure \ref{fig:WB1prof} there is a zoomed image of both these profiles.
The measured integral dose is $103.3\,cGy$ for MatRad $103.0\,cGy$ for Fine Sampling and $103.7\pm1.0\,cGy$ for Monte Carlo simulations.\\
\\

\begin{figure}[!ht]
\centering
\subfloat[][\emph{Central Dose profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_09.png}} 
\subfloat[][\emph{Integrated Depth Dose profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_10.png}} \\
\subfloat[][\emph{Phantom surface entering profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_11.png}} 
\subfloat[][\emph{Peak profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_12.png}}
\caption{Profiles in water box simulation.}
\label{fig:WB1prof}
\end{figure}

%\begin{figure}[!ht]
%\centering
%\subfloat[][\emph{Central Dose profile}]
%{\includegraphics[width=.45\textwidth]{Images/Images_homog_phantom/CP_wb_zoom.png}} 
%\subfloat[][\emph{Integrated Depth Dose profile}]
%{\includegraphics[width=.45\textwidth]{Images/Images_homog_phantom/IDD_wb_zoom.png}}
%\caption{Profiles in water box simulation, zoomed}
%\label{fig:WB1profz}
%\end{figure}
\newpage
\begin{figure}[!b]
\centering
\subfloat[][\emph{MatRad}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_15.png}}\quad
\subfloat[][\emph{Fine Sampling}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_16.png}}\quad
\subfloat[][\emph{Monte Carlo}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_17.png}}\quad
\caption{Dose deposition in water box, $30^\circ$ degree entering beam.}
\label{fig:WB2}
\end{figure}
\subsubsection{$30^\circ$ entering beam}
\begin{figure}[!t]
\centering
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_18.png}} 
\subfloat[][\emph{$\gamma$-index \\ $98.77\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_19.png}}
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_20.png}} 
\subfloat[][\emph{$\gamma$-index \\ $96.30\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_21.png}} 
\caption{Comparison between MatRad and Monte Carlo (left), Fine Sampling and Monte Carlo (right) in water box.}
\label{fig:WB2gam}
\end{figure}
\begin{figure}[!b]
\centering
\subfloat[][\emph{Integrated Depth Dose profile}]
{\includegraphics[width=0.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_22.png}}\quad
\subfloat[][\emph{Central Dose profile}]
{\includegraphics[width=0.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_24.png}}\quad
\subfloat[][\emph{Peak profile}]
{\includegraphics[width=0.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_25.png}}\quad
\caption{Profiles in water box simulation.}
\label{fig:WB2prof}
\end{figure}
The next case will be a beam entering the phantom with a $30^\circ$ angle. This example is important to understand the basic differences between MatRad and Fine Sampling algorithms. As can be seen from figure \ref{fig:WB2}, the shape of these dose depositions appears different compared with the Monte Carlo simulation. The reason why they behave differently comes from the evaluation of the radiation depth. MatRad, as a software based on \emph{Ray Casting}, calculates the radiation depth for every possible particle direction and adjusts the precomputed Bragg peak in water to the aforementioned radiation depth. Fine sampling instead divides the incoming particle fluence in several sub-beams and their radiation depth is evaluated singularly and only on the central axis of the sub-sample giving a better sensitivity to the morphology of the target but a bad response to small inhomogeneities inside the tissue.
This is the reason for a considerable shift of the Bragg Peak in the central and lateral dose profile of the peak, shown in figure \ref{fig:WB2prof}. I calculated the shift and I obtained $0.4\pm0.3\,mm$ for the Fine Sampling algorithm and $0.7\pm0.4\,mm$ for the MatRad algorithm. This can be considered a great result given the $3mm$ resolution of the CT data.
The total Dose deposited is $811.9$, $810.2$ and $813.2\pm8.1\,cGy$ for MatRad, Fine Sampling and Monte Carlo, respectively.

\begin{figure}[!h]
\centering
\subfloat[][\emph{MatRad}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_27.png}} 
\subfloat[][\emph{Fine sampling}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_28.png}}
\subfloat[][\emph{Monte Carlo}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_29.png}} \\
\subfloat[][\emph{Integrated Depth Dose profile}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_35.png}}
\subfloat[][\emph{Phantom surface entering profile}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_36.png}}
\subfloat[][\emph{Peak Profile}]
{\includegraphics[width=.31\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_37.png}}
\caption{Profiles in water box simulation and Profiles in water box simulation,  $45^\circ$ degree entering beam.}
\label{fig:WB3prof}
\end{figure}

\subsubsection{$45^\circ$ entering beam}
The last case is the $45^\circ$-degrees-entering-beam. This shows us, in a more explicit form, the issue of Ray-Casting-algorithms treating surfaces with particular shapes. In figure \ref{fig:WB3prof}, the first thing we can notice is the artifice in MatRad simulation that reproduces the angle shape of the CT surface in the dose deposition. This is clearly wrong, even if the Monte Carlo simulation outlines the same behaviour in a smoother way. Fine sampling produces a wrong interpretation caused by the weights assigned to the sub-beams. In other words, it gives to the central beam almost twice the weight of the nearest lateral sub-beams and almost 20 times more compared to the sub-components at $3\sigma$.
Even though this little issue does not compromise the accuracy of the Fine Sampling simulation, as confirmed from the comparison tests in figure \ref{fig:WB3gam} and figure \ref{fig:WB3prof}. From these figures, I measured a shift of $0.3\pm0.3\,mm$ for the Fine Sampling and $0.9\pm0.7\,mm$ for MatRad in the beam direction, and an integrated Dose of $946.8$, $940.3$ and $951.5\pm10.7\,cGy$ for MatRad, the Fine Sampling and the Monte Carlo, respectively.

\begin{figure}[!ht]
\centering
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_30.png}} 
\subfloat[][\emph{$\gamma$-index \\ $96.62\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_31.png}}
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_32.png}} 
\subfloat[][\emph{$\gamma$-index \\ $99.90\%$ [$1\%$ $1mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_homo/mR_FSBvsMC_pub_homog_33.png}} 
\caption{Comparison between MatRad and Monte Carlo (left), Fine Sampling and Monte Carlo (right) in water box.}
\label{fig:WB3gam}
\end{figure}

\subsection{Inhomogeneous Water Box}
\begin{figure}[!b]
\centering
\subfloat[][\emph{MatRad}]
{\includegraphics[width=0.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_02.png}}\quad
\subfloat[][\emph{Fine Sampling}]
{\includegraphics[width=0.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_03.png}\label{fig:IPgam_FS}}\quad
\subfloat[][\emph{Monte Carlo}]
{\includegraphics[width=0.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_04.png}}\quad
\caption{Profiles in water box simulation.}
\label{fig:IP}
\end{figure}

This phantom consists of a cube divided in two parts along the beam direction, one filled with water and the other one with a material with a slightly smaller electron density, that I will call \emph{gum}, for simplicity.
The aim is to show how Ray Casting algorithms sees this kind of phantoms.
Ray casting gives back a dose with a clear step in it, caused by the same mechanism discussed before. The comparison with Monte Carlo simulation gives a very good result for Fine Sampling. With matRad: we obained $95.72\%$ of passing points with a [$3\%$ $3mm$] criterion against the $97.68\%$ of the Fine Sampling, in the same conditions. This algorithm has a disadvantage in this experiment, because there is no smoothing in dose from the two sides of the phantom. The method I adopted in order to try to solve this issue is using a Fine Sampling model with a great number of sub-beams, in order to make them narrower and with a more balanced weight. I chose to bring the number of beams to 324 and, as you can see from the dose deposition in figure \ref{fig:IP} and from comparisons in figure \ref{fig:IPgam}, this improves the accuracy over MatRad level.
\begin{figure}[!ht]
\centering
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_05.png}} 
\subfloat[][\emph{$\gamma$-index \\ $95.62\%$ [$3\%$ $3mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_06.png}}
\subfloat[][\emph{Percentage\\ difference}]
{\includegraphics[width=.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_07.png}} 
\subfloat[][\emph{$\gamma$-index \\ $97.68\%$ [$3\%$ $3mm$]}]
{\includegraphics[width=.25\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_08.png}} 
\caption{Comparison between MatRad and Monte Carlo (left), Fine Sampling and Monte Carlo (right) in water box.}
\label{fig:IPgam}
\end{figure}

\begin{figure}[]
\centering
\subfloat[][\emph{Water central profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_09}\label{fig:IPprof_wcp}} 
\subfloat[][\emph{Plastic central profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_10.png}\label{fig:IPprof_pcp}}\\
\subfloat[][\emph{Integrated Depth Dose profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_11.png}} 
\subfloat[][\emph{Phantom surface entering profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_12.png}}\\
\subfloat[][\emph{Water peak profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_13.png}
\label{fig:IPprof_wpp}}
\subfloat[][\emph{Plastic peak profile}]
{\includegraphics[width=.45\textwidth]{Images/Images_inhomo/mR_FSBvsMC_pub_inhomog_14.png}
\label{fig:IPprof_ppp}}\\
\caption{Profiles in water box simulation and Profiles in water box simulation,  $45^\circ$ degree entering beam.}
\label{fig:IPprof}
\end{figure}

You may noticed that Monte Carlo simulation has two peaks having a not symmetric shape, with respect to the vertical axes. Instead, Fine sampling shows an almost full simmetric shape of the peaks. This phenomenon derives from the shape of the sub-components. Each of these is a Bragg peak; during the calculation they do not consider if they are crossing am inhomogeneity along the lateral direction. In a case like this, the sub-beams are divided in two categories: those that are attuenated inside one material, like water; and those attenuated in the other one. There are no middle ways.
Then, let's say that half of the components build one peak and half build the other one. In addition, the beams nearer to the center have a bigger weight, hence a stronger influence on the resulting shape of the peak.
Looking at the profiles\footnote{The profiles \ref{fig:IPprof_wcp} \ref{fig:IPprof_pcp} are the central dose profiles, on water side and plastic side. In particular, they are two parallel lines of voxels, at 1 voxel distance} in figure \ref{fig:IPprof_wcp} and \ref{fig:IPprof_pcp}, we notice this issue. It is visible that, in both cases, FS has a peaked maximum; this is not true for MC that smoothly decreases. matrad, because of its features, has a behaviour completely indipendent for the two profiles.
We see a complete agreement on one side of the first peak but a poor one on the second one.
This is highlighted also in $\gamma$-index map of FS (figure \ref{fig:IPgam_FS}) and in the lower peak profile\footnote{The profiles \ref{fig:IPprof_wpp} \ref{fig:IPprof_ppp} are the peak profiles, on water side and plastic side.} (figure \ref{fig:IPprof_ppp}).

About the Integrated Depth Dose Profile, the curve between the two peaks, in both MR and FS, has a sharp gradient change. Monte Carlo, on its own, is obviously smoother and the overflow of Dose is compensated with a smaller maximum. As proof, the total Dose is $116.2$, $116.0$ and $116.4\pm1.3\,cGy$ for matRad, FS and Monte Carlo respectevely.



\subsection{Airbox Phantom}
As introduced above, the airbox phantom is a water phantom with a small cubic air-pocket inside. This should test the behaviour of the softwares in situations similar to lungs and intestine treatment. 
I repeated the simulation six times, for six different air-pocket positions. It was placed:

\begin{enumerate}
\item Right on the surface, in figure \ref{}
\item at $5\,mm$ from the surface, in figure \ref{}
\item at $15\,mm$ from the surface, in figure \ref{}
\item at $25\,mm$ from the surface, in figure \ref{}
\item at $35\,mm$ from the surface, in figure \ref{}
\item at $45\,mm$ from the surface, in figure \ref{}
\end{enumerate}

% add figures

In these example, the key issue is the deposition of dose in air. The dose deposited in a certain volume of air is lower than the dose deposited in a similar volume of water. Beacuse of this reason, the Bragg peak in air is more extended in depth and this produces a smoother and lower peak. In figures \ref{fig:AB5prof_app} and \ref{fig:AB6prof_app}, one can see that there is not a peak inside the air-box for MR and MC, instead FS shows a clear peak. The issue is the same introduced in the previous section. The peak on the side without air-pocket, is extended laterally because of the modified ray casting introduced by fine sampling, considering the radiation depth, so the stopping power, of the central axis of the sub-component, only.
MatRad does not suffer much this problem because of the aforesaid method based on ray casting. It calculates the dose deposited in air as constant. In practice, it is just like if it traslates the bragg peak of a distance equal to the air-pocket width (particularly clear in figure \ref{fig:AB6prof_app}), divergence of the beam in air is still considered.
None of the two approaches (MR and FS) is correct, they create artifacts that considerably change the shape of the dose deposited in respect to reality.

From the lateral profiles, we can get more interesting information. The shape of the MC peaks, is well interpreted by FS for the first cases, in the last cases evolves and slowly approaches the shape of MR (especially on the side without air-pocket).

\subsection{Soukup Phantom}
As explained in section \ref{sec:pha}, I replicated a phantom used by Soukup (et al.) in \cite{souk:pba}. 
This phantom is not so far from the air-box one because it has a low density window that cuts the beam in halves. They main difference is the presence of a high-stopping-power region that can be thought as a bone. This has the aim of checking what happens in a high-gradient region, as in the inhomogeneous phantom, but restricted to a strip of determined thickness.

The experiment was made with two different energies of the beam (figure \ref{}), $125$ and $172\,MeV$. One that has the bragg peak inside the inhomogeneity and another one that crosses it and deposits the dose deeper.
Knowing the behaviour of the dose deposition in airbox phantom, the result of the $\gamma$-index value is not so impressive as it seems. When there is a dose peak in the inhomogeneity, fine sampling shows its limits.
Comparing the lateral profiles of the two cases on the peaks (figures \ref{}), we do not see big differences from one another. Instead from depth-dose profiles, it is clear that FS describes a whole different behaviour, especially in the second peak where dose maximum is almost half of what should be. Watching at the integrated depth-dose profile, there are small differences between the peaks, due probably to the fact that, in MC dose distribution, the dose in the knee region before the peak is basically a little higher that expected from the other methods (as already seen in waterbox case) and this phenomenon is emphasized in this case.


\newpage

\section{Spread-Out Bragg Peak}

\section{Realistic Treatment Fields}
\subsection{Double Wedge}



\subsubsection{Full Field}

In this section, I will expose the predictions made with MatRad and Syngo and I compare them by using the $\gamma$-index test. The figures shown are slices of a sagittal plane of the phantom passing through the isocenter of the CT-cube
\begin{figure}[!ht]
\centering
\subfloat[][\emph{Syngo}]
{\includegraphics[width=.45\textwidth]{Images/Images/epub_comparison2_01.png}} \quad
\subfloat[][\emph{MatRad}]
{\includegraphics[width=.45\textwidth]{Images/Images/epub_comparison2_02.png}} \quad
\caption{Simulated Double Wedges Dose deposition}
\label{fig:SyngoMat}
\end{figure}

a

\begin{figure}[t]
\centering
\subfloat[][\emph{Percentage Dose Difference}]
{\includegraphics[width=.65\textwidth]{Images/Images/epub_comparison2_03.png}} \\
\subfloat[][\emph{Gamma index test with parameters 1\% - 1$mm$}]
{\includegraphics[width=.65\textwidth]{Images/Images/epub_comparison2_04.png}} \\
\subfloat[][\emph{Gamma index test with parameters 2\% - 2$mm$}]
{\includegraphics[width=.65\textwidth]{Images/Images/epub_comparison2_05.png}} \quad
\caption{Comparison between MatRad and Syngo}
\label{fig:SyngoMat2}
\end{figure}

\subsubsection{Measurements}
The last step of my work is to check the compatibility of our prediction with the real measurements. These are our gold standard and this is the final test that every software has to overcome to be consider a good predictor.

As said in Chapter 2, the detector inside Double Wedges Phantom has been placed in 9 different positions and, for each of them, I have the dose value measured in every chamber and that one predicted by Syngo. In this case, the simulation with Syngo has been made in two different ways. The first simulates the whole proton beam and the dose deposition inside the CT-cube (it works in the same way as MatRad does), then from this cube the values of dose in the areas covered by every single chamber are extracted. The second one, referred as Syngo, evaluates the dose directly inside the area covered by every single ionization chamber; this causes a slightly difference between them.

In the following pages, I will show a series of images where there are shown a slice of the dose cube evaluated with MatRad and the projection of the position of the 24 chambers, all in CT resolution. Associated to this set of images, there is a graph that reports the actual value in $cGy$ of the ionization chambers for measurements and simulations.
\newpage
\begin{figure}[h!]
\centering
\subfloat[][\emph{Sagittal plane}]
{\includegraphics[width=.65\textwidth]{Images/Images/Meas_MR_comparison_01.png}} \quad
\subfloat[][\emph{Coronal plane}]
{\includegraphics[width=.65\textwidth]{Images/Images/Meas_MR_comparison_02.png}} \\
\subfloat[][\emph{Ionization chambers values}]
{\includegraphics[width=.65\textwidth]{Images/Images/Meas_MR_comparison_03.png}} \quad
\caption{Results, geometry and comparison from configuration 1}
\label{fig:pos1}
\end{figure}

\newpage
\begin{figure}[h!]
\centering
\subfloat[][\emph{Sagittal plane}]
{\includegraphics[width=.45\textwidth]{Images/Images/Meas_MR_comparison_01.png}} \quad
\subfloat[][\emph{Coronal plane}]
{\includegraphics[width=.45\textwidth]{Images/Images/Meas_MR_comparison_02.png}} \\
\subfloat[][\emph{Ionization chambers values}]
{\includegraphics[width=.45\textwidth]{Images/Images/Meas_MR_comparison_03.png}} \quad
\caption{Results, geometry and comparison from configuration 2}
\label{fig:pos2}
\end{figure}

\subsection{Alderson phantom}

\section{Overview} % Mean tables


\chapter{Discussion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}
In the previous chapter, we went through some comparisons for single bixel cases and for full field simulation between simulations run with pencil beam and pencil beam fine sampling algorithms, Monte Carlo methods and measurements obtained at Heidelberg Ion-Therapy center.
\begin{table}[b]
\centering
\begin{tabular}{ccccc}
\toprule
Case & \multicolumn{2}{c}{MatRad} & \multicolumn{2}{c}{Fine Sampling}\\
\midrule
 & $\gamma$-index pass rate & max $\%$ diff & $\gamma$-index pass rate & max $\%$ diff\\
\midrule
$0^\circ$ & $99.92\%$ & 1 & $99.92\%$ &1\\
$30^\circ$ & $98.77\%$ & 1 & $96.30\%$ &1\\
$45^\circ$ & $96.62\%$ & 1 & $99.91\%$ &1\\
\bottomrule
\end{tabular}
\label{tab:hom}
\caption{Results for waterbox phatom analysis}
\end{table}

First, I proposed the results for four different types of phantom irradiated with a single proton beam at fixed energy.
With the waterbox phantom, we proved that there is a very good agreement between matRad, fine sampling and Monte Carlo in the simplest case: the $0^\circ$ entering beam. Gamma-index test is over $99.9\%$ and the maximum percentage difference is less than $5\%$ for both the used methods, due to a light shift in depth.
The Bragg peak is not identical beacuse the look-up tables of matRad (and as a consequence, of fine sampling) are obtained using a Monte Carlo based software that works in a slightly different way than FLUKA.
In the other waterbox cases, we saw that matRad is still robust in describing the dose deposited by a beam entering with a $30^\circ$ angle from a flat surface and that failed the test of the $45^\circ$ entering beam because the beam is centered on the corner of the cube and then the peak is shaped in the same way. This is the main issue of ray-casting-based methods. Instead with fine sampling we have seen a good approximation in the $45^\circ$ case because it overcomes the problem of the corner, but poorer results in the other case because of the weights that are higher on the central sub-beams that on lateral ones. In table \ref{tab:hom}, a general overview for this case is reported.

In the homogeneous phantom we saw how the three methods behaves in cases where the beam encounters a steep lateral gradient (in stopping power) on its path. Dose calculated with matlab appears sharply separated on two sides and there is no smooth transiction between them. It is like they are two completely indipendent (half a) Bragg peak.
Fine sampling, instead, seems to have the opposite problem. An eventual transition through a lateral gradient is not taken in account, so the Bragg peaks generate by the sub-beams cross the plane between the two materials without being affected and this is reflected on the result, with two peaks wider than expected, accross the inhomogeneity. About gamma-index test, fine sampling obtained a slightly better result than matRad, with $97.68\%$ against $95.62\%$.

\begin{figure}[!t]
\centering
{\includegraphics[width=.83\textwidth]{Images/gammacase}} 
\caption{Gamma index in relation to the case number, for FS and MR related to tab \ref{tab:air}}
\label{fig:gamcase}
\end{figure}

In the airbox case, we proved what Schaffner (et al.) (\cite{schaf:pba}) infer on their paper. The fine sampling method is not able to describe correctly dose deposition when the Bragg peak (or part of it) is in an area where there is a high gratient in the CT. From case 5 on, the Bragg peak is on the air-pocket and we start seeing some issues. The gamma-index pass rate of fine sampling goes under the value of that one of matRad. It coincides with the situation in which the first Bragg peak is right on the air-pocket. As you can see in figure \ref{fig:gamcase} and in table \ref{tab:air}, the $\gamma$-index pass rate of FS simulation, decreases with the distance of the air-pocket from the surface. Instead it is the opposite for matRad that has its best agreement when fine sampling starts to fail. As explained before, the reason is the dose deposited in air, which is smaller than dose inside the tissue and this results in sharply separated beams, more similiar to matRad ones. We have many analogies between airbox and Soukup phantom. We again see the problems of fine sampling when it calculates the Bragg peak on high gradient area. In fact, on the same phantom, it scores a gamma-index pass rate of $96.15\%$ in case with the most energetic beam, against the $83.77\%$ of the other case, where part of one peak is the high gradient area of the phantom. In this case, the proble is not air as itself but the principle is the same.
As you know, the inhomogeneity is divided in two parts. The particles that cross the boarder from the higher to smaller stopping power parts, have still enough energy to fly deeper in the tissue so, there is only half of a Bragg peak appearing in the high stopping power region that does not have a corresponding one on the other side.

\begin{table}[t]
\centering
\begin{tabular}{ccccc}
\toprule
Case & \multicolumn{2}{c}{MatRad} & \multicolumn{2}{c}{Fine Sampling}\\
\midrule
 & $\gamma$-index pass rate & max $\%$ diff & $\gamma$-index pass rate & max $\%$ diff\\
\midrule
1 & $93.45\%$ & 1 & $98.30\%$ &1\\
2 & $98.77\%$ & 1 & $96.30\%$ &1\\
3 & $96.62\%$ & 1 & $99.91\%$ &1\\
4 & $96.62\%$ & 1 & $99.91\%$ &1\\
5 & $96.62\%$ & 1 & $99.91\%$ &1\\
6 & $96.62\%$ & 1 & $99.91\%$ &1\\
\bottomrule
\end{tabular}
\label{tab:air}
\caption{Results for airbox phatom analysis}
\end{table}

%%% double wedge??


\section{Interpretation}
What we generally see from all the data obtained is that the matRad and fine sampling behave in complete opposite ways. MatRad, beacuse of its intrinsic properties, calculates the dose deposition caring only about the longitudinal variation of the stopping power of the medium, i.e., in the direction of the beam. The strenght of this method is that .....................  . Its weakness is the sharpness of the distributed dose. In case of a considerable inhomogeneity (like bone or air, for example), the resulting dose is distinctly cut in separeted parts without any smooth transiction between them. Similar is the case of a beam entering a surface with a peculiar shape (as the $45^\circ$ waterbox case or the double wedge), where the Bragg peak is strictly conditioned to assume a clearly innatural shape.
Fine sampling on its own, tries to solve this issue and to obtain a smoother dose. This is very effective in case where the inhomogeneity is near the surface of the patient but less accurate in the case of a dose deposition in areas with steep longitudinal gradients in stopping power (e.g., the inhomogeneous phantom). The clearest example is the $\gamma$-index pass rate as function of the airbox distance from the surface (figure \ref{fig:gamcase}).

At the end, we can conclude that fine sampling is a method at least equivalent to matlab.......

in medio stat virtus.

\section{Other Published Data}

\chapter{Conclusions} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix
%\chapter{$\gamma$-index test}

\listoffigures
\listoftables

\begin{thebibliography}{99}

\bibitem{bang:apm}
Bangert M., Hennig P., Oelfke U. -
\emph{Analytical probabilistic modeling for radiation therapy treatment planning} - 
Phys. Med. Biol. 58 5401–5419 (2013)

\bibitem{boeh:fluka}
Böhlen T.T., Cerutti F., Chin M.P.W., Fassò A., Ferrari  A., Ortega P.G., Mairani A., Sala P.R., Smirnov G. and Vlachoudis V. -
\emph{The FLUKA Code: Developments and Challenges for High Energy and Medical Applications} -
Nuclear Data Sheets 120, 211-214 (2014) 

\bibitem{bort:bragg}
Bortfeld T. -
\emph{An analytical approximation of the Bragg curve for therapeutic proton beams} -
Med. Phys. 24, 2024-33 (1997)

\bibitem{bort:SOBP}
Bortfeld T. and Schlegel W. -
\emph{An analytic approximation of depth–dose distributions for therapeutic proton beams} -
Phys. Med. Biol. 41, 1331–9 (1996)

\bibitem{chu:pass}
Chu W.T. - 
\emph{Instrumentation for treatment of cancer using proton and light-ion beams} -
Rev. Sci. Instrum. 64, 2055–2122 (1993)

\bibitem{deg:lin}
Degiovanni A. -
\emph{Future trends in linacs} - 
CERN, Geneva, Switzerland (2017)

\bibitem{ferr:fluka}
Ferrari A., Sala P.R., Fassò A. and Ranft J. -
\emph{FLUKA: a multi-particle transport code} -
CERN-2005-10, INFN/TC\_05/11, SLAC-R-773 (2005)

\bibitem{garc:nucu}
García-Ramos J.-E., Alonso C.E., Andrés M.V., Pérez-Bernal F. -
\emph{Basic Concepts in Nuclear Physics: Theory, Experiments and Applications} -
La Rábida International Scientific Meeting on Nuclear Physics (2015)

\bibitem{gra:hu}
Grant R. L., Summers P. A., Neihart J. L., Blatnica A. P., Sahoo N., Gillin M. T., Followill D. S. and Ibbott G. S. -
\emph{Relative stopping power measurements to aid in the design of anthropomorphic phantoms for proton radiotherapy} -
J Appl Clin Med Phys. 15(2): 4523 (2014)

\bibitem{hab:scan}
Haberer T. - 
\emph{Magnetic scanning system for heavy ion therapy} -
Nucl. Instr. Meth. Phys. Res. A 330, 296–305 (1993)

\bibitem{haet:frag}
Haettner E., Iwasel H., Krämer M., Kraft G. and Schardt D. -
\emph{Experimental study of nuclear fragmentation of $200$ and $400\,MeV/u$ ${12}^C$ ions in water for applications in particle therapy}-
Phys. Med. Biol. 58, 8265–8279 (2013)

\bibitem{high:mcs}
Highland V.L. -
\emph{Some practical remarks on multiple scattering} -
Nucl. Instr. Meth. Phys. Res. 129, 497–499 (1975)

\bibitem{jett:SOBP}
Jette D. and Chen W. - 
\emph{Creating a spread-out Bragg peak in proton beams} -
Phys. Med. Biol. 56, N131–N138 (2011)

\bibitem{jong:iba}
Jongen Y., Abs M.,. Blondin A, Kleeven W., Zaremba S., Vandeplassche D., IBA (Belgium), Aleksandrov V.S., Gursky S., Karamysheva G.A., Kazarinov N.Yu., Kostromin S.A., Morozov N.A., Samsonov E.V., Shirkov G.D., Shevtsov V.F., Syresin E.M., Tuzikov A., JINR (Russia) - 
\emph{Current status of the IBA C400 cyclotron project for Hadron Therapy} - 
(2018)

\bibitem{Kra:range}
Kraan A.C. -
\emph{Range Verification Methods in Particle Therapy: Underlying Physics and Monte Carlo Modeling} - 
Front Oncol (2015)

\bibitem{Kram:track}
Krämer M. - 
\emph{Calculations of heavy-ion track structure} - 
Nucl. Instr. Meth. Phys. Res. B 105, 14–20 (1995)

\bibitem{low:gamma}
Low D. A., Harms W. B., Mutic S. and Purdy J. A. -
\emph{A technique for the quantitative evaluation of dose distributions} -
Med. Phys. 25, 5 (1998)

\bibitem{mol:mcs}
Molière G. -
\emph{Theorie der Streuung schneller geladener Teilchen II, Mehrfach- und Vielfachstreuung} - 
Z. Naturforsch. 3a, 78–97 (1948)

\bibitem{mye:acc}
Myers S.and  Bruning O. -
\emph{Challenges And Goals For Accelerators In The XXI Century} -
World Scientific

\bibitem{par:latspr}
Parodi K., Mairani A., Sommerer F. -
\emph{Monte Carlo-based parametrization of the lateral dose spread for clinical treatment planning of scanned proton and carbon ion beams} -
Journal of Radiation Research 54, i91–i96 (2013)

\bibitem{schaf:pba}
Schaffner B., Pedroni E. and Lomax A. -
\emph{Dose calculation models for proton treatment planning using a dynamic beam delivery system: an attempt to include density heterogeneity effects in the analytical dose calculation} - 
Phys. Med. Biol. 44, 27–41 (1999)

\bibitem{schn:mcs}
Schneider U., Besserer J., Pemler P. - 
\emph{On small angle multiple Coulomb scattering of protons in the Gaussian approximation} - 
Z. Med. Phys. 11,  110- 118 (2001)

\bibitem{serb:nucRea}
Serber R. - 
\emph{Nuclear reactions at high energies} -
Phys. Rev. 72(11), 1114–1115 (1947)

\bibitem{jd:llu}
Slater, J.D. -
\emph{Development and Operation of the Loma Linda University Medical Center Proton Facility} -
Technol Cancer Res Treat. 6(4 Suppl), 67-72 (2007)

\bibitem{souk:pba}
Soukup M., Fippel M. and Alber M. -
\emph{A pencil beam algorithm for intensity modulated proton therapy derived from Monte Carlo simulations} -
Phys. Med. Biol. 50, 5089–5104 (2005)

\bibitem{weyr:rbe}
Weyrather W.K., Ritter S., Scholz M., Kraft G. - 
\emph{RBE for carbon track-segment irradiation in cell lines of differing repair} -
capacity. Int. J. Radiat. Biol. 75, 1357–1364 (1999)


%% aggiungere history of hadrontherapy dell'amaldi e del degiovanni


\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO Section %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1) Change the figures with the correct ones and change the gamma index pass rate.

% 2) Add some new images for the Double Wedges phantom.

% 3) 


\end{document}
